So, ich bin am Start, alles wieder in Ordnung, ja geht, also es war gestern Abend echt, ist noch ein bisschen, ist noch schlimmer geworden, also ich war glaube ich insgesamt, ja ich muss glaube ich fünfmal aufs Klo oder so, bis es dann wieder gut war, bis ich dann keinen Bauchweh mehr hatte, also das weiß ich, was es war, ich hab das manchmal,
ja, meistens so ein paar Stunden nach dem letzten Essen, irgendwie manchen Tagen vertrage ich nix, weiß nicht woran das liegt, ja war gestern Abend blödes Timing, aber gut, was soll ich machen, ich mein, ich kann ja schlecht mit Bauchweh vorm Rechner sitzen und streamen, in die Hose kacken oder so, naja, das geht ja nicht, also insofern, keine Chance, komm, wir machen wieder ein bisschen Wupp Wupp von gestern an, Wupp Wupp, stehen geblieben, Wupp Wupp,
hast du heute Urlaub? Nein, ich hab heute keinen Urlaub, ich krieg das hier, den Stream als Arbeitszeit bezahlt, Quatsch Leute, Lull, ich arbeite, ich arbeite nur vier Tage in der Woche und montags hab ich immer frei, was sehr, sehr ist, nein, nein, ich hab keinen Urlaub,
ich hatte dieses Jahr noch gar keinen Urlaub, ich hab sogar den Abend,
ich hab den alten Urlaub mitgenommen vom alten Arbeitgeber zum neuen, der neue hat's noch nicht gut geschrieben, aber ich denke mal, das werden die im Laufe der Woche machen, Strategie nicht gut gewählt, ja, das stimmt, ich weiß, dass ihr gerade alle arbeiten müsst, aber was wäre besser, gar nicht jetzt, oder, was soll ich machen, ne, außerdem könnt ihr den Stream sicherlich als Fort- und Weiterbildung abrechnen, ja, so würde ich das auch machen, ich mein, hier lernt man ja was, wir gucken uns heute wieder Google Cloud mit Terraform an und ich zeige euch heute, ich mach jetzt das gleiche,
Intro nochmal, was ich, was ich gestern gesagt hab, so, äh, und zwar, ich zeige euch heute, wie's richtig geht, ganzen Dezember Urlaub, weiß ich nicht, ich hab mich schon, hab ich noch nicht geplant, das zählt fast als Arbeitszeit, dass wir, da wir uns auch, äh, Google Cloud im Geschäft anschauen, ja, das ist so, passt doch, passt auch ganz, passt auch gut, wieso, wie gesagt, wir machen ja hier wirklich Sachen, die euch auch auf der Arbeit was bringen, also, das ist ja nicht, muss es ja nicht komplett verheimlichen, man, gerade wenn ihr was mit Google Cloud und Terraform macht, dann ist es jetzt, es ist ja wirklich,
so was wie, wie, wie Weiterbildung, wenn man so will, ja, natürlich ist vielleicht der Chef ein bisschen skeptisch, wenn das auf Twitch-TV passiert, man, man weiß ja nicht, übrigens, mein, mein Ex-Kolleg, oder was, mein, mein Ex-Kolleg, der mit mir auf die neue Arbeit, der wieder mein richtiger Kollege jetzt, äh, ihr wisst ja, ähm, der hat heute auch angefangen, der hat wieder richtig die Arschkarte gezogen, der war irgendwie im Urlaub vorher und wollte eigentlich gestern da sein und heute anfangen auf der Arbeit und dann ist irgendwie sein Flieger ausgefallen,
er ist heute erst um elf oder so was in Frankfurt angekommen, macht super Eindruck am ersten Arbeitstag, oder, wenn du anstatt morgens wie verabredet erst um elf auftauchst, war das bei der Pizza letzte Woche nicht auch so, ich weiß es nicht, es wäre auf jeden Fall denkbar und wenn es bei der, immer bei der Pizza ist, dann sollte ich vielleicht abends keinen, es ist ja nicht so, ich esse ja gar keine ganze Pizza, ich esse meistens zwei, zwei, drei Stück, je nachdem, wie groß die sind, das ist auch so ganz dünne Pizza,
also, da ist eigentlich nichts,
nix dran, großartig, aber gut, es kann doch durchaus an abends Pizza liegen, dass ich Bauchweh hab, ja.
Bei meiner Arbeit gibt es einen DNS-Block für Twitch, hä?
Ach so, ein DNS wird geblockt für Twitch, okay, ich dachte, du hast einen Block für Twitch in deiner DNS-Configuration, hä?
Was?
Terraform Apply, wir machen mal Terraform Destroy.
Okay, wenn wir gerade dabei sind, ah, no, wir destroyen das nicht, es gibt nämlich einen Bug, den ich euch gleich zeigen muss, du kannst den Stream ja, du kannst den Stream ja, ja, Chat, Leute, ich meine, ich kann auch heute Abend streamen, wenn euch das lieber ist, aber, ich glaube, jetzt wieder aufzugehen ist auch doof, oder?
Ich meine, ihr könnt den Stream ja auch im Wort gucken, das ist ja immer noch irgendwie drei Monate auf Twitch, oder ihr geht auf archivwublos.tv, könnt euch das im Wort anschauen.
Oder ihr geht auf den Wort-Channel auf YouTube, also, wir müssen noch ein paar Sachen anpassen, was wir gestern gemacht haben, ich mach nochmal, ich mach nochmal ein bisschen Intro, ja.
Oh, nice, Pfeifert, ich bin nicht ausgelockt aus Google Cloud, das ist schon mal gut, da muss ich mich nämlich nicht wieder einloggen.
Unknown Error, Keckel, Keckel Test 1, 2, 3, perfekt.
Also, ich mach nochmal das Intro, von dem ich gestern gemacht hab, also wir haben ja vor zwei Tagen bisschen rumgebastelt.
Mit Google Cloud und Terraform, ich erzähl euch ja auch gleich nochmal kurz, was Terraform ist.
Vor allem, wenn wir da rumkonfigurieren, und da war ja ein bisschen das Problem, dass ich selbst, mir ist noch nicht so genau im Vorfeld angeguckt hab, wie man den Terraform Google Cloud Provider benutzt.
Also, sprich, wie man den Terraform Google Cloud provisioniert, und ich hab mir das jetzt ein bisschen angeguckt, und heute zeige ich euch, wie man das richtig macht.
Damit ich nicht alle Lorbeeren selbst ernte hier, ich hab mir ein YouTube Tutorial angeguckt.
Und von irgendeinem Anton, das hier, hab ich mir angeguckt, und hab mich davon ein bisschen inspiriert.
Also, wir machen es allerdings nicht genauso, wie er das macht.
Ich werd die Terraform Datei ein bisschen anders benennen, und auch ein bisschen anderen Inhalt reinschreiben.
Aber das war ein sehr gutes Tutorial für Terraform, Kubernetes Cluster, und so ein bisschen Basic Usage von Terraform plus Google Cloud.
Weil es ist doch schon anders als Azure, muss man sagen.
Terraform, jetzt nochmal zur Erklärung, was das Ganze ist.
Bei der Erklärung selbst von denen, ihrer Seite, blickt man ja nicht wirklich durch, was die einem sagen wollen.
Was ist Terraform?
Terraform ist ein Tool, mit dem man sich Cloud-Infrastruktur provisioniert.
Kann man sich jetzt vielleicht ein bisschen schwierig was darunter vorstellen.
Ganz praktisches Beispiel, wenn ihr bei irgendeinem Cloud-Anbieter, sei es bei Hetzner, sei es bei DigitalOcean,
sei es bei Google Cloud, das ist eigentlich vollkommen egal, wenn ihr dort beispielsweise VMs installieren wollt,
Netzwerke anlegen wollt, bei irgendeinem Cloud-Provider, dann könnt ihr das entweder von Hand im Web-Interface machen,
was halt nicht reproduzierbar ist, das müsst ihr dann jedes Mal machen und dürft keine Zwischenschritte vergessen und sowas.
Oder ihr macht das mit Terraform.
Terraform ist quasi ein Cloud-übergreifendes Tool, wo man Cloud-Ressourcen anlegen kann.
So dass es reproduzierbar ist und man nicht alles immer wieder von Hand machen muss.
Terraform hat noch ein weiteres cooles Feature, wobei das manchmal auch Probleme...
Ich mach die Musik aus.
Wupp wupp wupp.
Terraform der große Bruder von Ansible.
Ne, sind zwei unterschiedliche Tools.
Kann ich aber auch noch gleich was zu sagen, weil das gerne gefragt wird.
Also, um das jetzt hier nochmal abzuschließen.
Terraform hat noch ein weiteres sehr nice Feature.
Was wir vielleicht auch gleich sehen werden.
Und zwar, Terraform merkt sich, welche Infrastruktur man damit angelegt hat.
Und man kann damit die Infrastruktur auch wieder löschen.
Also sprich, wenn ihr Terraform benutzt, um bei Hetzner in der Cloud 10 VMs zu erzeugen.
Und ihr wollt die dann irgendwann wieder löschen, dann könnt ihr einfach sagen Terraform Destroy.
Das ist das Gegenteil von Terraform Apply.
Dann guckt es in seinen State rein und sieht, aha, ich habe vorher diese und diese VMs aufgesetzt.
Das sind also diese VMs mit diesen Inputs.
Die internen IDs und sowas.
Die muss ich jetzt auch wieder löschen.
Und wenn jemand von Hand sich einloggt und dann in der Cloud an den VMs rumspielt.
Und hier Terraform wieder ausführt.
Dann merkt Terraform, dass da einer was von Hand geändert hat.
Und bietet das an, das wieder rückgängig zu machen.
Also, es ist ein sehr nützliches Tool.
Es gibt noch mehr Tools in dieser Richtung.
Terraform ist nicht das einzige, sollte man sagen.
Auch sehr beliebt ist Pulumi.
Das ist von den Features her ungefähr...
...gleich mit Terraform.
Allerdings, man benutzt das mit einer handelsüblichen normalen Programmiersprache.
Also in Terraform definiert man seine Ressourcen so.
In so einer komischen...
Das ist kein YAML, das ist HCL nennt sich.
Das sieht ein bisschen aus wie YAML vielleicht.
In so einer extra Definitionssprache macht man das in Terraform.
Und in Pulumi macht man das zum Beispiel...
...in C-Sharp oder in Getting Started.
Gucken wir uns mal sowas an.
In Pulumi macht man das zum Beispiel...
Es wäre natürlich sehr nice, wenn man ein schönes Beispiel hätte.
Ja gut, genau hier sieht man es.
In Pulumi würde man das gleiche dann machen.
Bloß, dass man das zum Beispiel in JavaScript, Python, Go, C-Sharp...
...oder auch in so einem YAML-Dialekt machen kann.
Ist deutlich vielseitiger.
Pulumi als Terraform.
Dadurch, dass es eben ein Framework ist...
...für so die handelsüblichen Programmiersprachen, die es gibt.
Also wenn man ganz abgedrehte Sachen machen will...
...ist das wahrscheinlich mit Pulumi einfacher.
Dafür ist der Einstieg auch komplizierter.
Terraform hat das auch erkannt.
Wir machen jetzt mal so einen Rundum-Überblick.
Terraform hat das auch erkannt.
Und hat CDKTF ins Leben gerufen.
Das ist Terraform.
Allerdings nicht mehr...
...in ihrem komischen eigenen Dialekt hier.
In ihrem eigenen YAML-Dialekt.
Sondern Terraform auch in handelsüblichen Programmiersprachen.
Ist auch eine coole Geschichte.
Habe ich selbst noch nie benutzt.
Haben wir bei uns im To-Do drinne stehen.
Werden wir irgendwann demnächst mal ausprobieren.
Das Ganze, egal jetzt ob Terraform oder Pulumi...
...nennt man Neudeutsch Info.
Wobei Neudeutsch ist vielleicht ein bisschen verkehrt.
Wenn man hip sein will, ja.
Also auch auf Neuenglisch...
...nennt man das Ganze Infrastructure as Code.
Warum?
Man sieht es ja hier schon.
Man beschreibt nur noch...
...was in der Cloud angelegt werden soll.
Zum Beispiel so ein Netzwerk hier.
Mit dieser IP-Range.
Und Terraform übersetzt das dann...
...in die passenden Cloud-Ressourcen.
Man muss die Cloud-Ressourcen nicht selbst anlegen.
Also ich könnte auch zum Beispiel...
...hier bei Google in das Web-Interface gehen.
Und in Netzwerke.
Und dann sagen...
...create network.
Könnte ich auch machen.
Ich könnte es aber auch Terraform machen lassen.
Das heißt, dementsprechend...
...Infrastructure as Code.
Man beschreibt halt, was man haben will.
In Source-Code-Dateien.
Und Terraform...
...applied das dann, das nennen die auch so...
...auf die jeweilige Cloud-Umgebung.
Man könnte das auch für Azure machen.
Oder für DigitalOcean.
Oder für AWS oder sowas.
Wobei man auch sagen muss...
...Terraform...
...ist keine Abstraktionsschicht...
...zwischen den Clouds.
Also man definiert...
...in Terraform jetzt nicht...
...ein Netzwerk oder eine VM...
...oder ein Kubernetes-Cluster...
...jedes Mal gleich, egal ob für Azure...
...oder für Google.
Man muss schon die Cloud-spezifischen Sachen...
...machen, nur dass man es halt in Terraform...
...reproduzierbar machen kann und State-Tracking dabei hat.
Dann sollte man noch eine Sache...
...erwähnen. Wie gesagt, letztendlich...
...unter der Haube ist es eigentlich ziemlich wurscht...
...was man verwendet. Ob es jetzt Terraform ist...
...oder Pulumi ist.
Oder hier dieses neue Terraform...
...mit in Programmiersprachen verwendet.
Wichtig ist, dass das Team damit klarkommt...
...wofür man das benutzt...
...beziehungsweise in dem man ist...
...oder für die man das erstellt.
Anwendungsentwickler selbst mögen wahrscheinlich...
...ein bisschen lieber Pulumi.
Wohingegen die klassischen Admin-Teams...
...wahrscheinlich eher die...
...Terraform-Geschichte mögen.
Man muss sagen...
...Terraform ist das deutlich verbreitetere Tool...
...versus Pulumi.
Also Terraform ist viel, viel verbreiteter.
Terraform ist quasi so der...
...wie nennt man das so schön...
...der Industriestandard, falls es da sowas...
...gibt. Also Terraform ist das...
...bekanntere und das verbreitetere...
...von beiden Tools. Und wenn ihr Jobs sucht...
...ist Terraform-Knowledge...
...häufiger...
...gerne gesehen als Pulumi-Knowledge.
Wobei man halt auch sagen muss...
...wenn man Pulumi kann, kommt man relativ schnell...
...in Terraform rein und umgedreht.
Genau, soviel zum Thema Infrastructure as Code.
Jetzt mal zu der eigentlichen Frage...
...wie es mit Ansible aussieht.
Terraform und Ansible sind zwei...
...unterschiedliche Tools. Und zwar...
...Ansible...
...ist ein Tool um Server...
...zu konfigurieren und Terraform...
...ist ein Tool um Infrastruktur...
...anzulegen. Kann man sich quasi...
...so vorstellen, mit Terraform...
...erzeuge ich meine Cloud-Ressourcen...
...unter anderem auch VMs...
...und mit Ansible konfiguriere ich...
...die VMs dann, wenn es notwendig ist.
Der Trend geht ja dahin, dass...
...Cloud-Ressourcen möglichst...
...konfigurationsfrei sein sollten...
...möglichst immutable sein sollen...
...was natürlich Schwachsinn ist in der Realität...
...tritt das auch nicht ein...
...aber so...
...geht zumindest der Trend hin...
...was ja auch sinnvoll ist.
Also wenn man jetzt beispielsweise 20...
...virtuelle Maschinen in der Cloud erzeugen will...
...dann würde man das mit Terraform erzeugen...
...und wenn die noch etwas...
...spezielle, kompliziertere Konfigurationen...
...brauchen, dann würde ich das mit...
...Terraform konfigurieren...
...mit Ansible konfigurieren...
...nicht mit Terraform, mit Terraform provisionieren...
...mit Ansible konfigurieren...
...die Tools haben...
...gewisse Überschneidungen, man kann...
...teilweise Cloud-Infrastruktur auch...
...mit Ansible anlegen, man kann teilweise...
...ein bisschen so mit Inline Bash Scripts und sowas...
...auch mit Terraform dann...
...Sachen konfigurieren...
...sagen wir mal so 10-20%...
...überschneiden sich vielleicht...
...aber grundsätzlich sind das eigentlich schon...
...ziemlich unterschiedliche Tools...
...Terraform ist zum Anlegen, zum Provisionieren...
...und Ansible ist zum Konfigurieren...
...so, das ist so der grundlegende Unterschied...
...das heißt, wenn man...
...Cloud-Infrastruktur mit Terraform anlegt...
...ist sie noch nicht unbedingt in dem Zustand, wie man...
...das haben will, zum Beispiel auf VMs und da...
...wenn man es braucht, kommt dann sowas wie...
...Ansible ins Spiel, also beides...
...sehr nützliche Tools und auch...
...sowohl Terraform als auch Ansible...
...recht gefragt...
...wenn man sich mal so...
...ja so...
...Skill-Anforderungen anguckt...
...die Arbeitnehmer...
...die Arbeitgeber aktuell suchen...
...ist sowohl...
...Ansible als auch Terraform ganz gut dabei...
...Polumi, was Infrastructure as Code...
...angeht, ein bisschen weniger, wobei ich...
...persönlich, wenn ich für mich privat...
...machen würde, ja, ich persönlich mag...
...Polumi mehr als Terraform...
...also Geschmackssache und CDKTF...
...hab ich ausprobiert...
...soviel mal zur ganz schnellen...
...Übersicht, was Infrastructure as Code angeht...
...mag jetzt vielleicht ein bisschen abstrakt sein...
...wir gucken uns das gleich an...
...ich erzähle euch noch was dazu...
...ein Tool will ich aber jetzt...
...an der Stelle auch noch erwähnen...
...wenn wir gerade dabei sind...
...und zwar ist das Crossplane...
...das ist gerade im Kubernetes...
...aber da lasse ich mich jetzt nicht...
...großartig drüber aus...
...gerade im Kubernetes-Umfeld sehe ich da...
...in sowas in der Richtung die Zukunft...
...also sprich...
...Crossplane ist auch ein Tool...
...nur mal ganz zwei Sätze dazu, weil ich den Ansatz...
...wirklich sehr, sehr nice finde und ich hoffe...
...dass ich damit auch mal so...
...im Kubernetes-Umfeld was machen kann...
...wenn man...
...ehe plant...
...seine Sachen auf dem Kubernetes-Cluster zu deployen...
...dann kann man ja auch theoretisch...
...die Konfiguration anstatt über Terraform...
...auch...
...in Kubernetes-Config erstellen...
...also brauche ich keine Terraform-Files...
...und keine Kubernetes-Yammels, sondern ich kann für alles...
...Kubernetes-Yammels machen...
...und Crossplane bietet...
...dafür die Möglichkeit zum Beispiel...
...nur mal so jetzt...
...was man damit Cooles machen kann...
...dann kann man auch...
...eine Anwendung haben, die ich auf dem Kubernetes-Cluster...
...deployen will, die allerdings...
...nicht rein aus Kubernetes-Ressourcen...
...besteht, zum Beispiel...
...ich habe ein Kubernetes-Deployment...
...was im Prinzip ein Container-Image ist mit der Anwendung drin...
...das braucht aber zusätzlich noch...
...eine Datenbank, die nicht im Kubernetes-Cluster...
...laufen soll, zum Beispiel...
...hier Cloud SQL bei Google...
...ne, das ist die neue...
...hier Cloud SQL bei...
...Google Cloud...
...wie verbinde ich das Ganze jetzt zusammen?
Das ist so mit der klassischen...
...Infrastructure-as-Code...
...nicht so ohne weiteres möglich, du brauchst...
...einmal ein Kubernetes-YAML-Deployment...
...und du brauchst irgendwas, was deine...
...Cloud SQL-Datenbank...
...provisioniert...
...das heißt, du müsstest dann quasi...
...Terraform und...
...dein Kubernetes-Zeug irgendwie verbinden...
...geht, ja Flux...
...kann das mit einem...
...Terraform-Controller, aber...
...es ist nicht so schön, das ist das wo Crossplane...
...dazukommt, Crossplane...
...ist anders, du kannst quasi dann ein Deployment...
...konfigurieren für Kubernetes...
...und gleichzeitig allerdings auch im gleichen...
...Config-File oder so im gleichen Kontext...
...auch noch sagen, ich möchte bei Google...
...eine Datenbank provisionieren...
...ich sehe da persönlich die Zukunft...
...im Kubernetes-Umfeld eindeutig...
...bei Crossplane, weniger...
...bei Terraform, Terraform wird immer noch...
...wichtig bleiben, allein schon...
...zum initialen Aufbau und ganzer...
...Cloud-Umgebung, das wird man ja mit Crossplane...
...auch nicht machen...
...aber gerade das Handling von Anwendungen...
...und Deployments in Kubernetes...
...was externe Ressourcen zusätzlich...
...braucht, da sehe ich die Zukunft bei Crossplane...
...ich habe mir das in...
...letzten paar Wochen über schon ein bisschen angeguckt...
...noch nie großartig was mitgemacht, aber...
...das sieht wirklich gut aus, hat noch ein paar...
...Nachteile versus Terraform, aber ich...
...persönlich denke, da geht die Zukunft...
...hin, was...
...Kubernetes-Deployments angeht, aber das...
...gucken wir uns heute nicht an, heute machen wir den Klassiker...
...Terraform...
...den man auch auf absehbare Zeit...
...immer noch brauchen wird, also Crossplane...
...wird nie Terraform ersetzen...
...sondern eher im Kubernetes-Umfeld ein bisschen...
...die Sachen übernehmen, aber irgendwann...
...muss man ja mal initial den Cluster hinstellen, ja...
...ohne Kubernetes-Cluster kannst du ja auch...
...darauf nichts ausführen, und dafür wird es nach wie vor...
...für immer auf absehbare Zeit...
...zumindest wahrscheinlich Terraform oder Pulumi...
...oder sowas in der Richtung bleiben, das heißt...
...Terraform ist ein sehr wichtiger Skill...
...wenn man im...
...ja, so im...
...DevOps-Bereich, im...
...Server-Administrationsbereich, aber auch im...
...Entwicklungsbereich unterwegs ist...
...Terraform ist aktuell überall...
...so...
...soviel zur Einleitung...
...hab ich mir grad ein bisschen aus den Fingern...
...gesaugt, aber Chat, ich glaub man hat alles soweit...
...verstanden, oder? Ich verstehe, dass viele den Stream...
...auf der Arbeit nebenbei laufen haben und nicht soviel...
...schreiben können, Chat...
...ich glaub, konnte man halbwegs...
...ne, auf absehbare Zeit...
...ja, für immer ist nix...
...gerade in der IT, aber...
...Terraform hat sich so etabliert...
...dass ich nicht sehe, dass irgendwas...
...großartig Terraform in nächster Zeit...
...den Platz 1...
...schreitig machen wird, dahinter ist ein bisschen Pulumi...
...und...
...alles andere spielt eigentlich kaum...
...ne größere Rolle.
Kann ein Terraform mit Ansible zusammenarbeiten?
Ja, kannst du.
Ja, kannst du machen.
Okay, also die Host-Datei...
...die kannst du auch mit irgendeinem Inline Bash-Script...
...in Terraform...
...erstellen.
Also ich würd nicht...
...ich würd nicht Ansible...
...zusätzlich noch an Start bringen, nur um...
...die Host-Datei zu erstellen...
...auf VMs.
Aber du kannst selbstverständlich...
...Terraform und Ansible zusammen ausführen...
...gibt's ja mehrere Möglichkeiten...
...wie du das machen kannst.
Also es gibt ja bei Ansible, dass du das...
...pullen kannst...
...aus dem Git-Repo...
...oder aber auch von nem zentralen Ding...
...quasi pushen kannst auf die Kiste...
...zum Ausführen. Du könntest quasi zum Beispiel...
...am Ende von einer...
...von einem Terraform...
...Run, der dir VMs anlegt...
...könntest du in dem Cloud-Init-File...
...anstoßen, dass Ansible ausgeführt wird.
Und das geht. Das ist sogar sehr sinnvoll...
...zusammen zu verbinden, wenn man komplexere...
...Konfigurationsaufgaben für VMs hat.
Also wenn du wirklich öfters...
...VM anlegst mit Terraform, die dann...
...auch irgendwie ein bisschen komplizierter...
...eingerichtet werden müssen, als einfach nur...
...Standard-Image drauf...
...dann ist das sogar wirklich sinnvoll...
...Terraform und Ansible zu kombinieren.
Man kann Ansible...
...die Kiste konfigurieren, Terraform hat's ja vorher...
...angelegt.
Kann ich mit Terraform standardisierte AD-Gruppen anlegen?
Habe ich noch nie gemacht, weil ich mit Windows nichts zu tun habe...
...aber ich bin mir ziemlich sicher, dass das geht.
Original scharf, dankeschön...
...für den Sub.
So, so sieht's aus.
Und ich zeige euch heute, wie man das ordentlich macht...
...in Terraform für Google Cloud.
Wir werden heute folgendes machen.
Moment, ich muss mal kurz was im Chat noch beantworten.
Wie kommuniziert Terraform mit Ansible?
Gar nicht.
Also, achso, ja, okay...
...jetzt weiß ich, was du machen willst.
Wie kannst du das Ansible Inventory...
...erstellen mit Terraform?
Ja, da musst du dir...
...das würde ich gar nicht so sehr machen, also...
...es gibt keine direkte...
...also, es gibt vielleicht mittlerweile irgendwelche Plugins oder so...
...keine Ahnung.
Also es gibt jetzt standardmäßig keine direkte Kommunikation...
...zwischen Terraform und Ansible.
Es gibt verschiedene Sachen, wie du das machen kannst.
Du willst ja quasi dein...
...Ansible Inventory...
...nicht das Haus, weil du willst dein...
...Ansible Inventory mit Terraform erstellen.
Ja gut, so direkt...
...gibt es da glaube ich nichts eingebautes.
Was du machen kannst in Terraform, das...
...gucken wir uns vielleicht auch an, nachher...
...du kannst Outputs definieren.
Das sind quasi Werte, die...
...Terraform, nennen wir es mal, exportiert.
Also zum Beispiel...
...du legst eine VM an...
...mit Terraform und dann kannst du als Output...
...die IP und den Namen...
...von dieser VM...
...die sie bekommen haben nach dem Anlegen, mit Terraform...
...wieder exportieren und das...
...könnte dann in irgendeiner Art und Weise...
...Ansible bei dir benutzen, um sich...
...in der Inventory zu bauen, wenn man das so machen will.
Schöner wäre die Variante...
...allerdings...
...wenn du das unabhängig voneinander machst...
...weil du willst ja auch nicht unbedingt...
...immer Terraform am Start haben, wenn du...
...Sachen konfigurieren willst.
Dann sind wir jetzt aber schon im...
...Bereich ja quasi...
...Discovery, Service Discovery, Inventory...
...Discovery und...
...Resource Management, dass du quasi irgendwo...
...einen Key-Value-Store hast oder...
...irgendwo was, wo das registriert wird...
...initial, die Kisten, die du aufgebaut hast.
Ja, oder...
...aber die einfachere Variante...
...ist, also es gibt zwei Varianten, die noch deutlich...
...simpler sind. Du benutzt einfach...
...das jeweilige Cloud...
...Kommandozeilen-Tool von dem...
...Cloud-Anbieter, was du verwendest.
Ja, du kannst ja zum Beispiel relativ easy...
...hier mit GCP...
...GCP Compute...
...äh, nee, was ist es?
Ach, weiß ich gar nicht.
Ähm...
...kannst du dir deine VMs anzeigen lassen.
Ist das Addresses List?
Ich hab noch das falsche Projekt ausgewählt.
Ha, muss man gleich mal ändern.
Also kannst du ja auch mit dem jeweiligen Cloud-Tool...
...des Anbieters, was du ausgewählt hast, auflisten...
...was dann dort Terraform für VMs...
...erzeugt hat.
Ist besser, da musst du die zwei Sachen nicht miteinander...
...verdrahten.
Ja, oder...
...die allerbeste...
...Lösung, wenn möglich, bei dir...
...ist, du lässt einfach...
...nach einem Terraform Run...
...automatisch die neu erstellte VM...
... äh, Ansible ausführen.
Du kannst ja in Terraform...
...Inline-Script angeben, so eine...
...Zeile, gibst du einfach Ansible Run...
...oder Ansible Playbook...
...dann kannst du ein Git-Repo angeben...
...das Ansible automatisch applied wird...
...nach jedem Starten einer neuen VM.
Das ist auch möglich. Musst du halt überlegen...
...was bei dir am besten passt.
So, äh, ich muss mal das...
...Default-Projekt festlegen, kurz bevor...
...wir hier anfangen.
So, kommen wir jetzt aber mal zu Terraform und Google Cloud.
Ich hab ein Projekt angelegt schon...
...Cackle Stream 1...
...äh...
...what the hell?
Was hab ich hier reinkopiert?
Ach, die Projektnummer...
...ich hab das falsche kopiert, lol.
Ja, aber das sind meistens...
...Provisioners, die sollte man wirklich nur als...
...letzte Instanz wählen.
Ich weiß nicht, auf was sich das...
...bezieht jetzt gerade.
Du magst Recht haben, aber ich weiß nicht, was du meinst.
So, deine Meinung...
...ich hab damit noch nichts gemacht.
Ich weiß, dass das irgendwie so ein...
...bisschen containerzentriertes...
...OS ist, so wo alles...
...abgeschottet in Containern läuft.
Glaube zumindest, dass es das war.
Oder bringe ich das jetzt durcheinander?
Weil so, dass das irgendwas ist mit eigenem Package Manager...
...dafür alles containerisiert, hab ich noch nicht ausprobiert.
Klingt Security-technisch...
...nach einer sehr nicen Sache...
...und auch um Dinger rückstandslos zu entfernen.
Ich hab es noch nicht ausprobiert.
Keine Ahnung, kann ich nichts weiter zu sagen.
Ach, ist das nicht das mit den...
...was war denn das mit den Containern?
Ja stimmt, das könntest du machen lassen.
Wobei die schönere...
...jetzt weiß ich, was er meint.
Die schönere Variante ist ja irgendein Cloud-Init-File...
...zu nehmen, was das macht.
Ich würde auch nicht Terraform unbedingt...
...warten lassen, sondern...
...irgendein Cloud-Init reinhauen...
...und wo beim Start dann am Ende Ansible ausgeführt wird.
Kann ja Terraform...
...ich würde gar nicht Terraform wissen lassen...
...dass Ansible noch läuft.
Also sprich...
...wozu sollte...
...das Terraform wissen? Terraform ist fertig...
...sobald die VM über die API angelegt ist...
...und running ist.
Danach kann Ansible machen, was es will.
Also ich würde gar nicht so sehr...
...Kommunikation zwischen Ansible und Terraform probieren.
Denn es sind ja zwei unterschiedliche Tools...
...die auch nicht zwangsläufig von einer abhängig sind.
So, also...
...fangen wir mal an. Ich habe ein Projekt...
...angelegt in der Google Cloud...
...mit dem sprechenden Namen...
...KekkelStream1.
Wenn ihr ein Projekt in der Google Cloud anlegt...
...zeige ich euch...
...grad mal ein, weiß nicht ob ich es Trick nennen soll...
...aber ich nenne es mal Trick.
Normalerweise ist es so...
...wenn ihr hier irgendwas anlegt...
...so blubb, ihr könnt das Ding...
...Projektname blubb...
...und dann erstellt Google für euch...
...eine Projekt-ID, die weltweit...
...einzigartig ist.
Allerdings muss ich sagen...
...ist das Handling...
...von diesen IDs...
...ja...
...vielleicht...
...bisschen kryptisch, wenn man die an manchen Stellen...
...eintragen muss.
Wenn man die Möglichkeit hat...
...hier oben was einzutragen, was unique ist...
...dann seht ihr...
...dann generiert das keine Projekt-ID.
Also man muss sich überlegen...
...was einem wichtiger ist...
...ein cooler...
...kurzer Projektname...
...und dafür aber eine...
...nicht zu merkende ID...
...oder vielleicht ein etwas längerer Projektname...
...aber der weltweit...
...einzigartig ist...
...und ihr könnt den überall verwenden...
...ich find's zum Beispiel ganz cool...
...wenn man dann sowas macht...
...man kann ja zum Beispiel sowas machen...
...Wubblors Stream 1...
...und sowas in der Richtung...
...da ist relativ sicher davon auszugehen...
...dass das weltweit unique ist...
...ich weiß, es gibt jetzt bestimmt irgendwelche Leute...
...die mir auf den Sack gehen wollen...
...und Wubblors Stream 1, 2, 3, 4 und so schon bei sich registrieren...
...einfach nur, dass es angelegt ist...
...ich kenn euch...
...aber...
...normalerweise finde ich das die schönere Variante...
...weil dann ist es irgendwo...
...sprechend und ohne...
...kryptische ID hinten dran...
...natürlich...
...könnt ihr die Dinger dann nicht sowas nennen wie...
...default oder so...
...das ist logischerweise schon belegt...
...gibt...
...mehrere Leute auf der Welt...
...die schon den Projektnamen...
...default verwendet haben...
...also...
...unser Ziel heute ist folgendes...
...wir...
...ich mach nochmal von Anfang an...
...was wir gestern gemacht haben...
...ist ja nahezu nichts...
...ich zeig's einfach nochmal...
...also was wir heute machen ist folgendes...
...wir richten Google Cloud in Terraform ein...
...wir legen Netzwerke an...
...wir legen...
...einen Router an...
...wir legen NAT-Regeln an...
...also NAT-Regeln an, dass wir ins Internet kommen...
...wir legen...
...Firewall-Regeln an, dass wir per SSH...
...drauf zugreifen können...
...was machen wir noch...
...wir legen eine...
...Jump-VM an...
...dass wir uns quasi von daheim...
...in unsere private Google Cloud connecten können...
...wir legen...
...einen...
...Kubernetes-Account an...
...wir legen ein Kubernetes-Cluster an...
...und ein Kubernetes-Node-Pool an...
...allerdings...
...nicht public...
...das ist alles in einem privaten Netz...
...also wir werden das Netz hier verwenden...
...das ist alles in einem privaten Netz...
...das heißt man kommt vom Internet aus nicht dran...
...ist glaube ich sogar ein bisschen billiger...
...weil man keine public IPs braucht...
...kostet ja alles Geld...
...ich gehe mal ganz stark davon aus, dass es nicht umsonst ist...
...ich weiß es aus dem Kopf nicht, wie es in Google Cloud ist...
...aber höchstwahrscheinlich sind public IPs nicht umsonst...
...das heißt...
...das ist alles nicht aus dem Internet...
...erreichbar, das heißt man muss über so einen...
...Zwischenschritt...
...dass man zu seinem Cluster hinkommt...
...was nicht bedeutet...
...dass der Cluster nicht von außen...
...erreichbar gemacht werden kann...
...für Services, die im Internet...
...angeboten werden sollen...
...aber der Cluster selbst...
...also die Management Interface von dem Cluster...
...und von den VMs als...
...die sind nicht aus dem Internet erreichbar...
...Services, die auf dem Cluster laufen...
...irgendwelche Webseiten oder Webservices...
...die können aus dem Internet per Ingress erreichbar sein...
...aber ich denke mal, das werden wir heute nicht machen...
...so weit werden wir nicht kommen...
...wie gesagt, ich sage es nochmal...
...ich habe mir das nicht alles selbst ausgedacht...
...ich habe gestern ein richtig gutes...
...Tutorial Video geguckt...
...von ihm hier...
...sonst hätte ich das auch nicht so schnell gecheckt...
...wie man die Sachen gut in Terraform macht...
...aber wie gesagt, ich habe ein paar Sachen angepasst...
...und ein paar Dinge fehlen aus meiner Sicht auch...
...die man machen sollte...
...gut, auf geht's...
...Windows, was Windows, wo Windows...
...ach, du meinst hier Windows...
...ja, ich habe ein Setup aus...
...Linux Kiste hier und Windows Kiste hier...
...also ich habe für...
...Videos schneiden...
...Videos auf YouTube hochladen, Streaming...
...Sachen spielen...
...und für alles andere habe ich meine Linux Kiste am Start...
...oder ist das ein gutes KDE Stream...
...nö, das ist Windows...
...das ist Windows und das ist Linux...
...i3...Arch by the way...
...sollte ich vielleicht öfters mal erwähnen...
...weil die coolen Leute, wenn sie Arch verwenden...
...erwähnen das auch öfters...
...ich glaube...
...noch cooler wäre man nur mit Gentoo...
...oder Gentoo...
...wie das manche...
...aussprechen...
...gut, also fangen wir nochmal ganz...
...von Anfang an an...
...ich werde mir mal ein paar Sachen...
...Leute...
...wir müssen noch eine Sache von gestern fixen...
...sonst muss ich ein neues Projekt anlegen...
...und dann fangen wir nochmal von vorne an...
...hier an der Stelle...
...ich mach das jetzt einfach mal...
...dann erklär ich...
...dann erklär ich...
...dann erklär ich was es macht...
...wenn wir soweit sind...
...Terraform apply...
...so mehr haben wir noch nicht...
...und dann machen wir das nochmal rückgängig...
...und dann fangen wir von vorne an...
...so ja, yes, wunderbar...
...ok, gut...
...Terraform destroy...
...so man hat jetzt vielleicht...
...ohne dass ich das jetzt im Detail erklärt habe...
...was passiert ist...
...man sieht jetzt zum Beispiel schon mal was richtig cooles...
...Terraform...
...kann Ressourcen anlegen...
...und kann, also mit Terraform apply...
...sachen anlegen...
...ich hoffe ich hab mein Projekt jetzt nicht kaputt gemacht...
...weil ich das vorhin letztes mal vergessen hab...
...ne ok...
...er scheint zu funktionieren...
...Terraform merkt sich welche Ressourcen das angelegt hat...
...und kann die Dinger danach auch wieder löschen...
...das ist sehr praktisch, also wenn ich...
...ich kann quasi was aufbauen...
...und rückstandslos wieder entfernen...
...also eigentlich eine ganz coole Geschichte...
...wenn man zum Beispiel in einem Projekt was aufbaut...
...wo man nicht danach, wenn man fertig ist...
...das ganze Projekt in der Cloud wieder löscht...
...aber ich zeige es euch...
...ich zeige es euch im Detail...
...so ich hoffe das klappt jetzt...
...Network delete...
...er sollte alles löschen...
...so und ich kopiere mir jetzt noch zwei Scripts...
...die ich mir gebastelt habe rüber...
...das macht es nämlich ein bisschen einfacher...
...das erste ist ein Initscript...
...legen wir hier mal rein...
...ah nein es hat funktioniert...
...ähm...
...geke Terraform...
...so ich lösche mal kurz...
...meinen ganzen Terraform State...
...so als hätten wir noch nichts gemacht...
...Terraform Logs und alles...
...so dann machen wir noch ein extra Verzeichnis...
...nennen wir mal Terraform...
...und alles was es hier gibt...
...reinschieben...
...weil wir müssen es ein bisschen besser strukturieren...
...denn hier kommt ja später noch ein Kubernetes...
...ordner hin...
...und alles mögliche...
...wir machen jetzt mal hier ein Script hin...
...das nennen wir init...
...und ich kopiere mal was rein...
...und dann führen wir das mal aus...
...das macht es nämlich ein bisschen einfacher...
...so also...
...ich kann euch nicht sagen ob das der Weisheitsletzter Schluss ist...
...das so zu machen...
...so wenn ich ein neues Google Cloud Projekt...
...anfange...
...bereite ich das ein bisschen darauf vor...
...dass ich mit Terraform meine ganzen Ressourcen...
...anlegen kann...so und zwar...
...ich suche mir einmal die aktuelle Projekt-ID...
...raus...dann lege ich ein...
...Storage Bucket an...
...zeige ich euch gleich wofür das notwendig ist...
...und ich lösche das Default...
...die Default Firewall Regeln und das Default Netzwerk...
...ans Trinken erinnern...sehr gut...
...exzellent...schau dir mir...
...Terra Mix an...
...Terranigma kenn ich...
...achso für...
...ja es gibt ja ein paar Terraform Rapper...
...ich bin immer noch für...
...Raw Terraform...
...so dann führen wir das jetzt mal...
...aus...
...achso das alte Storage...
...Bucket können wir auch noch löschen...
...Cloud...
...Storage...
...warum ist das so fett...
...Alter...
...das da...
...ja...
...so löschen wir mal das Storage Bucket...
...und dann fangen wir an...
...Delete...
...zack...
...okay führen wir das In-Hit Script aus...
...wie gesagt ich mach nachher noch ein Github Repo...
...wo das alles drin ist...
...auf löschen...
...please...
...yes...
...so das dauert jetzt kurz...
...und dann können wir uns schon ein bisschen um die...
...Terraform Sachen kümmern...
...dieses In-Hit Script habe ich mir...
...einfach nur gemacht damit es schneller geht...
...aus einem Default Projekt...
...das so zu bauen das ich Terraform ausführen kann...
...also das erste was man für...
...Terraform konfigurieren muss ist folgendes...
...man muss angeben...
...welche Provider man benutzen will...
...wir benutzen Google...
...das heißt wir benutzen den Terraform Provider für Google...
...wer hätte das gedacht...
...dann muss man noch einstellen welches Projekt...
...man...
...befüllen will...
...und ihr seht ja hier Kekkel Stream 1 ist mein Projekt...
...und dann noch in welcher Region wir das ganze...
...machen wollen...
...wir wollen das in der Region...
...Europe West 3 machen...
...Europe West 3 ist Frankfurt am Main...
...wenn man nicht weiß...
...was es ist...
...Europe West 3...
...man kann hier nachgucken bei Google...
...z.B. hier...
...Europe West 3...
...da sieht man...
...Europe West 3 ABC...
...das sind die Zonen btw...
...Europe West 3 ist Frankfurt Germany...
...aktuell zumindest...
...das hier ist die Region...
...und das hier unten drunter...
...dieses minus ABC...
...das sind Zonen...
...und manchmal muss man bei Google Cloud...
...entscheiden...
...möchte man Sachen in einer Region...
...oder in einer Zone aufbauen...
...wenn man das in einer Region aufbaut...
...dann werden manchmal...
...z.B. wenn man Storage oder sowas anlegt...
...dann wird das automatisch repliziert...
...oder die Kubernetes...
...Node...
...wie heißt...
...man repliziert automatisch in...
...alle Zonen einer Region...
...das kostet mehr...
...ist nice...
...will man aber oftmals gar nicht haben...
...d.h. man sollte sich immer überlegen...
...möchte man was regional...
...oder wirklich nur was in einer Zone aufbauen...
...und in einer Zone es aufzubauen...
...ist meistens billiger...
...weil regional bedeutet in der Regel...
...in allen 3 Zonen einer Region...
...ja und es gibt noch bei...
...Storage auch die Möglichkeit...
...so das ist eine ganz simple...
...Config was ich eigentlich...
...wofür ich Terraform...
...in dem Fall hier eigentlich verwenden will...
...ich will das für...
...ich soll euch vielleicht mal das...
...Terraform-Config-File zeigen...
...ich will das ganze für die Projekt-ID...
...KekkelStream1 verwenden...
...Region ist Europe-West3...
...und die Zone wenn ich Zonen verwenden...
...möchte dann möchte ich...
...die Unterregion...
...-a verwenden...
...und die Unterregion in dem Fall ist halt...
...die Zone, so nennt sich das bei Google Cloud...
...und...
...wenn man hier so ein File anlegt...
...Terraform.tf war es...
...dann zieht Terraform hier den Inhalt...
...dieser Variablen aus diesem...
...Variabelfile, das ist ganz nützlich...
...es kann ja durchaus sein, dass ich die gleiche...
...Terraform-Config...
...es kann ja durchaus sein, dass ich...
...die gleiche Terraform-Config...
...für mehrere...
...Projekte...
...benutzen möchte...
...dann könnte es sein, dass ich zum Beispiel sowas habe wie...
...CackleStreamDev...
...CackleStreamLive oder sowas...
...und dass ich das darauf dann applyen will...
...also ist das ganz sinnvoll...
...so die grundlegenden Sachen so wie Region...
...Zone und vor allem...
...Projekt-ID...
...variabel zu machen...
...man könnte sich das übrigens auch komplett sparen...
...man könnte zum Beispiel hier auch...
...sagen Default...
...und dann könnte ich hier mein Projekt eintragen...
...was ich aber ein bisschen blöd finde...
...weil es könnte ja sein, dass ich was anderes...
...verwenden will und aus versehen es dann in diesem Projekt...
...ausführe, obwohl ich das vielleicht gar nicht will...
...also ist...
...so finde ich es auf jeden Fall schöner...
...bis jetzt passiert noch nichts großartig...
...wir haben jetzt nur den Google Provider konfiguriert...
...und paar Basic-Variablen...
...angelegt, die wir in Zukunft brauchen...
...der nächste Schritt ist das hier...
...ja, da kann man sich jetzt überschreiten...
...macht man das in der extra...
...Datei oder macht man das hier beim...
...Provider drinnen...
...was das macht ist...
...die Voraussetzung dafür, dass wir...
...auch was anderes machen können, nämlich...
...das aktiviert in meinem Projekt erstmal...
...bestimmte Services...
...weil wenn ich jetzt hier reingehe...
...wobei ich habe es gestern schon...
...aktiviert, ich kann es euch nicht zeigen...
...normalerweise, wenn ich jetzt hier reingehe...
...in die Kubernetes Engine oder...
...in die Compute Engine...
...Compute Engine ist bei Google Cloud quasi alles...
...was mit virtuellen Maschinen zu tun hat...
...dann würde jetzt normalerweise hier was aufploppen...
...Achtung, dieser Service ist nicht aktiviert...
...sie müssen diesen Service erst aktivieren...
...so und nachdem ich...
...das ich eben...
...den Compute Engine verwenden will...
...kann ich hier sagen, Compute Engine...
...Service...
...aktivieren...
...und das hier ist wichtig an der Stelle...
...ich weiß nicht ob es ein Bug ist...
...oder ob es...
...by Design ist...
...man kann über...
...Terraform Services die man...
...aktiviert hat für das Projekt nicht wieder...
...deaktivieren...
...ich bin mir auch gar nicht sicher...
...ob man Services wieder deaktivieren kann...
...mit Web Interface...
...glaub ich habe ich es noch nie gemacht...
...geht wahrscheinlich auch irgendwo...
...also wenn ich jetzt beispielsweise für dieses Projekt...
...Compute Engine komplett wieder deaktivieren will...
...dann wüsste ich gar nicht wo ich es mache...
...und wenn man das hier einstellt...
...und ich mein...
...Terraform Projekt wieder entferne...
...dann lässt es das ganze einfach enabled...
...das ist quasi nur...
...anschalt only...
...ausschalten gibt es nicht mehr...
...das ist bei Services wichtig...
...weil ansonsten schmeißt...
...Terraform irgendwelche Fehler...
...also wenn ihr komische Fehler bekommt bei Terraform Destroy...
...belegt euch vielleicht ob ihr bei...
...Servicedefinition Disable on Destroy...
...ausschalten müsst...
...das dient aber alles noch...
...dazu das vorzubereiten für das was wir...
...machen wollen als es noch gar nichts...
...großartiges passiert...
...der nächste Block...
...ist wichtig...
...wenn ihr Terraform nicht nur alleine verwenden wollt...
...überlegt euch mal...
...ihr verwendet Terraform im Team...
...die Configs hier benutzen...
...mehrere Leute gleichzeitig...
...ich habe ja gesagt Terraform speichert...
...sich was es gemacht hat...
...standardmäßig...
...speichert Terraform das Lokal...
...bei euch im Verzeichnis...
...was...
...ich kack noob...
...mein Initscript funktioniert ja noch gar nicht so...
...ok...
...warum eigentlich nicht...
...achso weil ich das falsche Projekt eingetragen habe oder...
...ja ich noob habe da dev...
...eingetragen alter...
...nicht dev...
...naja egal...
...und wenn man jetzt mit mehreren Leuten...
...verwenden möchte...
...dann ist der State...
...den Terraform standardmäßig hier...
...bei euch lokal im Dateiverzeichnis...
...im Filesystem speichert...
...wird dann zu einem Problem...
...weil es könnte jetzt ja sein...
...dass ich das gerade ausführe...
...bei mir lokal und jemand anderes bei sich lokal...
...ausführt...
...dann haben wir zwei Terraform States die voneinander nichts wissen...
...das muss...
...über kurz oder lang zu Problemen führen...
...weil Terraform merkt sich...
...was es gemacht hat...
...und versucht es immer rückgängig zu machen...
...wenn jemand außerhalb von Terraform was dran gemacht hat...
...und wenn jemand Terraform auf einem anderen Rechner...
...laufen lässt...
...und die beiden voneinander nichts wissen...
...dann denken beide der andere hätte es ihm kaputt gemacht...
...das heißt man muss sich überlegen...
...wo man den Terraform...
...wo man den Terraform State speichert...
...und den Terraform State...
...kann man zum Beispiel in einem Storage Bucket...
...speichern in der Cloud...
...auf dieses Bucket greifen dann alle Leute...
...gleichermaßen zu...
...und es gibt keinerlei so...
...Synchronisationsprobleme und...
...einer macht dem anderen den State kaputt...
...das heißt da können mehrere Leute Terraform ausführen...
...und...
...es ist...
...alles gut...
...funktioniert...
...ja macht das...
...heute nicht arbeiten Max oder ist montags frei...
...ich habe montags frei immer...
...arbeiter habe ich montags frei...
...so ich muss mal kurz hier in meine Buckets gehen...
...ich habe glaube ich irgendwas erstellt...
...was ich nicht erstellen wollte...
...weil ich blöd rumeditiert habe...
...ist doch alles da...
...eigentlich alles gut...
...warum ist mein Innenscript gefailt...
...ah gut keine Ahnung...
...ich führ es so nochmal aus...
...nö geht doch alles...
...alles gut...
...also...
...wie wird bei GCP der Stateload gemanagt oder passiert das auch...
...ja...
...ich habe ein Bucket...
...wenn du das über Cloud Storage machst...
...hast du überhaupt keine Probleme mehr...
...wenn mehrere Leute...
...das heißt...
...überhaupt keine Probleme ist bei Terraform immer so ein Ding...
...das kann immer irgendwie außer Sync geraten...
...weil irgendwelche komischen Sachen passieren...
...aber du solltest keine Probleme mehr haben...
...wenn du den kompletten Terraform State...
...in dem Bucket speicherst...
...du musst das übrigens auch nicht...
...hier zwangsläufig...
...so machen wie ich das gemacht habe...
...die unterstützen mehrere Backends...
...also sie unterstützen alle...
...großen Cloud Anbieter...
...jeweils die...
...das spezielle von denen...
...aber auch S3...
...da bin ich jetzt bescheuert wo ist das denn...
...each Backend...
...available Backends...
...also die können natürlich...
...die können Google Cloud...
...die können natürlich auch Azure...
...und die können auch S3...
...du kannst es auch in Postgres speichern...
...als Secret im Kubernetes Cluster...
...webserver was auch immer...
...oder auch in Console...
...oder auch ganz ganz toll in der Alibaba Cloud...
...ne du kannst...
...du kannst bei AWS auch in einem S3 Bucket speichern...
...wenn du willst...
...du musst das nicht über DynamoDB machen...
...kannst du auch...
...ach Moment...
...Sekunde ich habe nichts gesagt...
...ich habe Mist erzählt...
...this Backend also supports...
...State Locking and Consistency Checking...
...ok...
...vielleicht habe ich dir auch Mist erzählt...
...vielleicht habe ich dir auch Mist erzählt...
...vielleicht habe ich dir auch Mist erzählt...
...this Backend supports State Locking...
...ne doch geht...
...das geht alles über das Storage Bucket...
...in dem Fall...
...das funktioniert...
...das kann man alles so machen...
...wenn man mehrere Umgebungen hat...
...dass man zum Beispiel DevInt...
...oder was weiß ich...
...DevLive...
...oder was auch immer hat...
...kann man hier noch Unterordner machen...
...wenn man unterschiedlich speichern will...
...ja...
...muss man sich ein bisschen angucken...
...was der jeweilige Cloud Anbieter unterstützt...
...und jetzt können mehrere Leute...
...parallel Terraform ausführen...
...ohne das was kaputt geht...
...so weit so gut...
...es gibt eine Sache die man noch machen sollte...
...die wir jetzt aber nicht gemacht haben...
...man kann hier...
...bei diesem Storage in der Google Cloud...
...einstellen dass das eine History...
...sich speichert...
...in dem Fall der Fälle das irgend jemand Mist baut...
...und was kaputt macht...
...könnte man so den State wieder...
...hat man quasi so ein automatisches...
...7 Tage Backup vom Terraform State...
...falls irgend jemand Mist eingetippt hat...
...und ich möchte es einfach schnell wieder rückgängig machen...
...machen wir jetzt an der Stelle nicht...
...is Overkill brauchen wir nicht...
...so und wenn wir das alles eingerichtet haben...
...können wir anfangen unser Google Cloud Projekt...
...anzulegen...
...das war alles nur Zeremonie jetzt...
...zur Vorbereitung...
...und wir fangen jetzt an...
...also wie gesagt...
...wir legen Netzwerke an...
...Routen an, Firewall Regeln an...
...Jump VM in die Cloud an...
...paar Accounts an, Kubernetes Cluster...
...und Kubernetes Cluster Node Pool...
...wir fangen allerdings ganz simpel an...
...und führen es mal aus...
...und dann versuche ich mal so ein bisschen grob zu erklären...
...was Terraform eigentlich macht...
...und warum es nützlich ist...
...und was das coole ist...
...wenn man seine Infrastruktur mit Terraform...
...verwaltet...
...damit wir unsere ganze Cloud Infrastruktur...
...zu anlegen ist das aller erste was wir brauchen...
...ein Netzwerk...
...und ich bin mal so frei und benenne das um...
...und zwar ein Network...
...und wir fassen das ein bisschen zusammen...
...wir fassen nämlich Subnetz...
...und Netz Network zusammen...
...hab ich mir überlegt...
...ist glaube ich einfacher zu verstehen...
...was letztendlich passiert...
...also...
...hier sagt man...
...mittels Terraform...
...an Google Cloud...
...wollen wir ein Netzwerk erstellen...
...mit der IP Range...
...oder mit der...
...SIDR...
...SIDR...
...das ist Classless Inter Domain Routing...
...kein Mensch...
...spricht das jemals so aus...
...wie auch immer...
...man könnte es einfach IP Range nennen...
...aber in dem Fall ist es IP Sidr Range...
...es gibt eigentlich...
...heutzutage so gut wie...
...warum sollte man andere Sachen benutzen...
...ich weiß es nicht...
...das ist die Range...
...die wir für unsere IPs benutzen...
...das heißt...
...10.0.0.0 slash 24...
...wir können mal gucken...
...ich meine es ist in dem Fall wirklich extrem simpel...
...leute...
...wer hat aufgepasst in der Berufsschule...
...was ist die erste IP...
...und was ist die letzte IP...
...die wir benutzen können in der Range...
...was ist die erste IP...
...und was ist die letzte IP...
...nutzbare IP...
...nutzbare IP wohlgemerkt...
...also 10.0.0.0 slash 24...
...die erste IP ist was...
...HiIQ Chat...
...jetzt enttäuscht mich nicht...
...es ist sehr simpel...
...das kann jeder Azubi in der dritten Woche...
...eins richtig...
...und was ist die letzte nutzbare IP in diesem Netz...
...254...
...exzellent...
...also das ist quasi das hier...
...bis...
...254...
...korrekt...
...Wir haben hier IP Calc...
...sehr nices Tool...
...Hosts Minimum...
...1 Host Maximum...
...254...
...und Broadcast Adresse ist 255...
...das heißt nutzbar in diesem Netz...
...sind 254 IPs...
...und die Netzadresse...
...ist das da...
...und also die Netzadresse ist das hier...
...und die Netmaske in dem Fall ausgeschrieben...
...das da...
...also das ist glaube ich das ziemlich...
...simpelste IP Netz was man überhaupt haben kann...
...oftmals...
...verwendet man daheim ja eher...
...das hier oder sowas hier...
...aber wir machen jetzt einfach mal das hier...
...hat den Vorteil werdet ihr später auch noch sehen...
...ein Netz zu verwenden was ihr...
...nicht zu Hause benutzt weil...
...wenn ihr euch dann später...
...so nennen wir es mal VPN mäßig...
...in eure Cloud verbinden wollt...
...ist es praktisch wenn es...
...dieses Netz bei euch lokal nicht gibt...
...weil ansonsten habt ihr das Problem...
...dann könnt ihr entweder nur auf das Netz lokal...
...bei euch zugreifen oder auf das Netz in der Cloud...
...zugreifen weil das ist das gleiche Netz...
...oder ihr müsst sehr viele komische Dinger machen...
...es ist sinnvoll...
...ein Netz zu benutzen was man...
...lokal nicht hat...
...zumal man hier sehr viele Möglichkeiten hat...
...das ist glaube ich standardmäßig in Slash 8...
...was dafür vorgesehen ist...
...als private IP Range...
...wobei ist es das überhaupt...
...ich glaube es ist standardmäßig in Slash 8...
...was man also man kann quasi das hier...
...das hier und das hier frei...
...frei benutzen...
...ja geht wieder besser...
...habe ich hier irgendwelche...
...Pepo Pupu Fire Toilet angesagt...
...Servus Max...
...weißt du ob man auf Linux Guzzi Sharp .NET...
...ja beste Umgebung dafür...
...wie kommst du drauf...
...dass es vielleicht nicht möglich sein sollte...
...das ist die beste Entwicklungs...
...beste Entwicklungsplattform für .NET mittlerweile...
...besser als Windows...
...die beste Plattform...
...um .NET Programme .NET Core...
...beziehungsweise es ist ja eigentlich nur noch .NET...
...heißt es jetzt...
...Dotnet 6, 7, 8...
...Programme entwickeln will ist das beste...
...was man machen kann auf Linux Basis...
...es sei denn man möchte eine Windows Only...
...GUI das ist korrekt...
...aber warum sollte man eine Windows GUI unter Linux...
...erstellen wollen...
...es gibt Möglichkeiten mittlerweile relativ gut...
...Cross-Plattform GUIs mit C Sharp zu erstellen...
...leider ist Cross-Plattform GUI immer noch...
...ziemlich Pains Champ...
...auch der Grund warum sich Elektron...
...so durchgesetzt hat die letzten Jahre...
...Avalonia ist...
...pretty POG...
...wie geht es dir...ja wieder besser...
...probiert kann man es nicht nennen...
...ich habe mal ein paar Beispielsachen...
...durch geklont von denen...
...von der Webseite...
...also nicht großartig was gemacht...
...alles klar...
...genau...
...wir legen jetzt Netzwerke in der Google Cloud an...
...ist Elektronik...
...nur eine Web Anwendung...
...eine Windows Only GUI...
...eine Windows Only GUI...
...unter Linux entwickeln ist schwierig...
...da kannst du es ja nicht ausprobieren...
...also wenn du Windows Only Software...
...entwickelst dann würde ich die...
...logischerweise auch unter Windows entwickeln...
...wenn du Avalonia...
...Cross-Plattform GUI benutzen...
...möchtest die würde dann unter Linux...
...und Windows laufen...
...aber testen musst du sie nach wie vor...
...unter Windows...
...also wenn du Windows Only...
...Software entwickelst dann mach das unter Windows...
...was anderes macht auch keinen Sinn...
...wie gesagt es sei denn benutzt du...
...Cross-Plattform GUI Framework...
...und...
...hoffst das es unter Windows dann auch so aussieht...
...wie das was du unter Linux testest...
...unter Windows programmieren...
...macht halt überhaupt keinen Spaß...
...aber wir müssen bei Google Cloud...
...Config bleiben sonst wird das heute nichts...
...also...
...wir legen jetzt Netzwerke in der Google Cloud an...
...also wir haben ein relativ...
...großzügiges Netz...
...254 Hosts haben wir hierfür...
...und...
...es gibt ein paar Sachen die man hier angeben muss...
...die ganz praktisch sind...
...oder besser gesagt...
...wenn es ein bisschen schwierig wird...
...und zwar das erste ist hier Depends On...
...wir möchten Netzwerke erst anlegen...
...nachdem alle APIs...
...aktiviert wurden...
...im Google Cloud Projekt was wir brauchen...
...das müssen wir aber nur einmalig machen...
...dass wir sicherstellen können...
...bevor wir anfangen irgendwas zu machen...
...dass die nötigen APIs im Projekt...
...eingeschaltet sind...
...das kann 20-30 Sekunden dauern...
...und dementsprechend ist das einmal sinnvoll...
...als Abhängigkeit zu definieren...
...so...
...ist erstmal wurscht wie das Ding heißt...
...ist auch erstmal wurscht ich benutze es...
...ich nenn es mal übrigens um...
...ich nenn es mal Default...
...weil ich habe das Default Netzwerk gelöscht...
...das finde ich viel angenehmer...
...Routing Mode...
...ihr könnt übrigens wenn ihr die Terraform...
...Visual Studio Code Erweiterung installiert habt...
...könnt ihr euch die Hilfetexte aus den jeweiligen...
...Cloud API Docs anzeigen lassen...
...sprich was hier möglich ist...
...und hier steht auch so ein kleiner Hilfetext dabei...
...und das es als mögliche Values...
...Regional und Global gibt...
...weil Routing Mode für unser Netzwerk...
...Regional ist vollkommen okay...
...Auto Create Subnets würde ich auch ausschalten...
...ich will die Sachen selbst anlegen...
...und das hier...
...finde ich tatsächlich relativ...
...nützlich...
...und zwar...
...Delete Default Routes on Create...
...setze ich auf False...
...das ist ganz nützlich dafür...
...wenn man zum Beispiel mit seinen VMs...
...ins Internet raus will um Paketupdates zu machen...
...braucht zwar glaube ich noch eine NAT Regel...
...bin ich mir jetzt gar nicht ganz sicher...
...aber...
...wenn man das komplett abgeschottet isoliert machen will...
...dann würde man das auf True setzen...
...wenn man aber zum Beispiel mit seiner VM...
...noch ordentlich ins Internet will...
...und eine Default Route haben möchte...
...dann würde ich das hier ausschalten...
...und hier unten...
...legen wir das eigentliche Netzwerk an...
...also das hier ist im Prinzip nur so ein...
...container...
...in dem man dann verschiedene Subnets...
...anlegen kann...
...man sieht es ja auch hier, dass man hier erst die IP Range angibt...
...nicht wie erwartet...
...mit dem Netzwerk selbst...
...sondern erst bei Subnet...
...so und das ist im Prinzip alles was ich euch schon erzählt habe...
...wir legen ein Subnet an...
...mit der IP Range...
...quasi die hier bis die hier...
...in der Region...
...die wir vorher als Variable definiert haben...
...also quasi hier in der Region...
...in der Region Europe West 3...
...und sagen dieses Subnet...
...gehört zu diesem Network...
...und...
...das ganze soll Private sein...
...ach ne Moment...
...das ist das die VMs Google...
...API Services...
...drauf zugreifen können, ja das ist sinnvoll...
...das schalten wir ein...
...und das wars, das wars für die Netzwerk Config...
...mehr müssen wir an der Stelle nicht machen...
...was jetzt passieren wird ist folgendes...
...wenn wir jetzt mal hier in Netzwerke gehen...
...übrigens bei Google Cloud...
...nennen sich Netzwerke VPC...
...ich glaub Virtual Private Cloud...
...
... weshalb man das nicht einfach nur Networks genannt hat...
...entzieht sich meiner Kenntnis...
...trotzdem muss man sagen...
...Google Cloud hat finde ich...
...noch ganz gute Benamungen...
...nicht so wie AWS...
...wo man echt nicht weiß wie die Sachen heißen...
...bei Google Cloud finde ich heißen die Dinger...
...meistens relativ gut benannt...
...Cloud Storage, SQL...
...Compute, Kubernetes...
...kann man sich alles was darunter vorstellen...
...bei Amazon heißt das dann...
...Fargate...
...und solche Geschichten...
...wobei es auch EKS...
...zumindest das was ich bisher von Amazon...
...gesehen habe...
...bei Google sind die Dinger ganz ok...
...Route 53 ist doch eindeutig...
...ich meine zumindest Port 53 und DNS...
...bringt man halbwegs zusammen...
...aber die Benahmung bei AWS finde ich...
...so ihr seht wir haben keinerlei Netzwerke...
...angelegt...
...das heißt wenn wir unsere Terraform jetzt ausführen...
...wir haben übrigens noch was vergessen...
...aber ich lege erstmal das Netzwerk an...
...dann zeige ich euch was noch fehlt...
...sollten wir danach ein Netzwerk haben...
...und zwar mit dieser IP Range...
...und das ist Terraform...
...Apply...
...so wenn man Terraform Apply macht...
...ups...
...was...
...ja ich sollte es auch vielleicht Default nennen...
...und nicht die Sache von gestern...
...wenn man Terraform Apply macht...
...dann zeigt einem Terraform immer erst an...
...bevor es anfängt...
...was es machen würde...
...das heißt Terraform hat sich jetzt connected...
...zur Google Cloud...
...übrigens wer sich fragt wie connected sich Terraform...
...überhaupt zur Google Cloud...
...das geht über das...
...Google Cloud Command Line Tool...
...wenn man das eingerichtet hat...
...das heißt bevor man das macht...
...muss man einmal gcloud init machen...
...und gcloud...
...äh ne auth...
...ja...
...gcloud auth application default login...
...Projektname bla bla bla...
...also man muss es einmal einrichten...
...und sich einmal anmelden...
...und dann kann Terraform das ganze über das Google Cloud Utility machen...
...das heißt man muss in Terraform...
...und in seinem Git Repo...
...wo man seine Terraform Files hat...
...keine Tokens und Passwörter hinterlegen...
...sofern derjenige...
...ein...
...eingerichtetes Google Cloud...
...Command Line Tool bei sich...
...auf dem Rechner hat...
...also wenn man sagt Terraform Apply...
...dann sagt einem Terraform erstmal was es machen würde...
...bei IBM...
...heißen die Dinge glaube ich auch VPC...
...bei IBM...
...IBM verkaufen einem auch tolle Großrechner...
...die dann bei einem im Rechenzentrum stehen...
...der vielleicht gar nicht gehört...
...IBM macht viele Sachen...
...wobei die aller schlimmsten sind immer noch Oracle was das angeht...
...was Lizenzierung und...
...vire Verträge angeht...
...ich glaube beim Oracle kommt da nix ran...
...wobei ich habe gehört SAP soll auch nicht ohne sein...
...ich bin froh das ich nie große...
...berührungspunkte bisher mit SAP hatte...
...und ich habe es ehrlich gesagt auch nicht vor...
...wenn man Terraform...
...Apply macht dann...
...erzählt einem Terraform erstmal was es machen würde...
...da kann man jetzt gucken ob das...
...passt was man da...
Also ob das so sein soll.
Terraform ist jetzt zu meiner Google Cloud in das Projekt gegangen und hat nachgeguckt, was ist denn schon da und was soll das anlegen.
Und jetzt schickt es mir ein Report und sagt, okay, ich würde folgende Sachen machen.
Ich lege ein Netzwerk an und ich lege ein Subnet an mit diesen Eigenschaften.
So, da kann ich jetzt durchgucken und alles ist gut.
Ja, und es enabelt unser Projekt, unsere APIs.
Dann sage ich, okay, sieht gut aus. Yes.
Wer keinen Bock hat, darauf zu warten und sich sicher ist, dass er immer fehlerfrei seine Terraform-Config erstellt, der kann sagen, auch wenn ich es nicht unbedingt empfehlen würde, der kann sagen, Terraform Apply minus Auto Approve.
Und dann sagt es automatisch Yes.
Ich weiß nicht, wie ihr das handhabt, Leute.
Seid ihr eher im, ich gucke mir alles an und sage Yes oder seid ihr eher im Auto Approve Lager?
Für die Leute, die schon ein bisschen Terraform benutzen oder Terraform öfters benutzen.
Ich muss sagen, ich bin faul as fuck.
Das heißt, ich benutze ganz gerne Auto Approve.
So, Terraform ist durch.
Wo läuft denn Terraform in der Praxis auf einer VM?
Ah, da kann ich gleich was zu sagen.
So, Terraform ist durch und wir sehen, dass es ein Netzwerk gibt.
Mit dem Namen Default, mit einem Subnet.
Und das ist das Subnet, was wir angegeben haben.
Also, das hat schon mal funktioniert.
So, und jetzt.
Das ist das Feature schlechthin, warum man Terraform benutzt.
Oder zwei.
Zwei Features schlechthin, warum man Terraform benutzt.
Das erste habt ihr gesehen.
Man hat jetzt alles, was man hier hat, definiert in der Textdatei.
Das heißt, wenn ich ein nächstes Projekt anlege und das genauso konfigurieren will, würde ich die Variablen austauschen.
Würde sagen, Terraform Init, Terraform Apply und es wäre genau in dem Zustand, wie es sein soll.
So.
Ähm, ich sag erst noch mal was hierzu.
Also, wo läuft denn Terraform dann in der Praxis auf einer VM?
Terraform kann ohne Probleme bei dir auf dem Laptop laufen.
Oder aber in einer VM.
Oder in einer Cloud VM.
Oder theoretisch glaube ich sogar in einer Cloud Shell.
Ähm, du kannst allerdings auch einen Workflow in GitLab oder in GitHub machen, der für dich Terraform Applied.
Da gibt es ein ganz cooles Projekt.
GitHub Terraform.
Atlantis.
Wenn man im Team arbeitet, ist es tatsächlich ganz nice.
Und zwar ist das eine Pull Request Integration für Terraform.
In einer, ja, kannst du es auch in einer Pipeline ausführen.
Genau.
Ja, oder GitHub Actions, was ja im Prinzip so eine, ist ja eine Pipeline.
Ja, wenn man das im Team hat, äh, im Team benutzt und das ein bisschen strukturierter haben will, dann kann man Atlantis benutzen und Atlantis hängt sich quasi in diesen Team.
Und Atlantis hängt sich quasi in diesen Team.
Und Atlantis hängt sich quasi in diesen Pull Request Workflow rein.
Ich weiß nicht, ob es ein gutes, ein gutes, ja, hier sieht man es eigentlich ganz gut, ja.
Open Pull Request.
So, und dann schreibt ihr Atlantis quasi rein, was würde jetzt passieren mit einem Terraform Plan.
Terraform Plan ist das, was das anzeigt, was es machen würde.
So, und dann können auch andere Leute diesen, diesen Output von dem Plan reviewen und approven.
Und wenn das gemerged wird, so, dann, wenn das gemerged wird, dann.
Dann wird das Ganze, ach ne, man applyt es erst und dann mergt man das.
Okay, auch gut.
Dann ist das Ganze ein bisschen nachvollziehbarer.
Wo ist der Unterschied zu Docker mit Docker Compose?
Docker ist die, dein Command Line Tool, um Container zu starten, zu stoppen, down zu loaden und anzulegen.
Und Docker Compose ist was, um mehrere Docker Images miteinander zu verbinden.
Also du kannst mit Docker Compose.
Alles das machen, was du auch von Hand mit Docker machen kannst, nur halt in einer automatisierten Art und Weise.
Und unterm Strich sind alle diese Tools, egal ob es Docker ist, Potman, Kubernetes oder was auch immer, benutzen die alle die gleiche Funktionalität.
Ja, das kann man nicht oft genug sagen.
Die, also die Möglichkeit Container auszuführen, Container gibt es ja eigentlich gar nicht, habe ich ja schon oft genug gesagt.
Das sind Kernelfunktionen von Linux.
Und das besteht hauptsächlich aus zwei Potman, aber Pogman wäre auch, gibt es, gibt es mal ein Gitter.
Okay, es gibt schon auf Gitter Projekte, die sich Pogman nennen.
Ja, also nochmal, nochmal zu dieser, zu dieser Container Geschichte, immer schnell was gesagt.
Letztendlich sind das zwei Funktionalitäten zum größten Teil, die Linux anbietet und die ausnahmslos alle Container Runtimes benutzen.
Weil es gibt keine Container, außer das.
Was der Linux-Kernel anbietet, sind zwei Sachen.
Das sind C-Names und Namespace, äh, C-Groups und Namespaces, so rum.
Das eine ist so ein bisschen Sandboxing und das andere ist, in, dass man quasi so tun kann, als wären Sachen anders.
Zum Beispiel andere IP, andere Hostname, als es wirklich ist.
Unter der Haube gibt es keine Container.
Es gibt Sandboxe Prozesse mit C-Groups und Prozessen, den du vorstellst.
Stockerhaukels, der Hostname wäre anders mit Namespaces.
Aber unter der Haube, es gibt eigentlich keine Container in dem Sinn.
Sondern, das ist Linux-Kernel-Funktionalität, die um ganz normale Prozesse drumherum gelegt wird.
Also, ob ihr jetzt zum Beispiel Echo 1, 2, 3 so ausführt oder Docker, Run, ähm, ja, Ubuntu, Echo, Echo 1, 2, wie geht das nochmal, Ubuntu?
Ich weiß nicht, ob das sowas so funktioniert.
Ne, wie, wie, wie führt man ein Kommando drinnen aus?
Da muss ich Bash, muss ich Bash minus C machen?
Weißt du, ey, ich hab keinen Plan aus dem Kopf, wie es funktioniert.
Muss ich, nee, ich muss Exec machen, oder?
Ähm, jetzt kann ich mir aus dem Kopf nicht merken.
Exec, hier, das, das, das muss ich machen.
Aber ich will Runnen und direkt Exec machen.
Warte mal, wie ging das? Minus It?
Das will ich machen, das will ich machen, okay.
Okay.
Also, ob ich jetzt sowas hier mache, ähm, kann man jetzt hier minus C, Echo 1, 2, 3 machen?
Ja, das funktioniert.
Okay, also, ob ich jetzt sowas hier ausführe, oder ob ich hier sowas hier ausführe, da ist weniger Unterschied, ich, im Detail natürlich schon, ist weniger Unterschied dazwischen, als man vielleicht so denkt.
Das hier wird auf meinem Host-System ausgeführt, genauso wie das, es wird beides auf meinem Host-System ausgeführt.
Der Unterschied ist, hier benutzt es Binaries von Ubuntu.
Ubuntu, um es auszuführen, aber unter der Haube sind das beides Prozesse auf meinem Host.
Das ist nicht wie bei VMs.
Also, da ist weniger Unterschied dazwischen, als man denkt.
Das hier ist kein Ubuntu im eigentlichen Sinne, das ist nach wie vor der ganz normale Kernel, der hier auch läuft.
Ähm, so.
Wenn ich jetzt, wenn ich mir hier anzeigen lasse, was ich, was ich, da bin ich mal gespannt, was eigentlich in der VM passiert, wenn ich das ausführe.
Äh, in der, im Container.
Was ist denn da passiert, wenn ich das ausführe?
Ja, da steht, da steht Mist da, der nicht, der nicht wirklich stimmt.
Doch, hier seht ihr es doch, guckt mal.
Also, euch wird vielleicht was auffallen bei den Sachen.
Hier lasse ich mir die Kernel-Version anzeigen auf meinem Host.
Hier lasse ich mir die Kernel-Version im Container anzeigen, guckt mal da.
Es ist der gleiche Kernel.
Also, ob ich was im Container ausführe, oder ob ich was auf meinem Host ausführe, das ist gar kein so großer Unterschied.
Das eine ist ein Prozess, der direkt losläuft.
Hier, auf meinem Host.
Und das andere ist ein Prozess, der auch direkt losläuft, nur dass noch ein bisschen Sandboxing-Zeug drumherum kommt.
Es sind beides Prozesse auf meinem Host.
Im gleichen Kernel.
Das ist der große Unterschied von Containern zu VMs.
Container gibt es in dem Sinn nicht.
Container ist ein Sammelsurium aus verschiedenen Sachen, die der Linux-Kernel bereitstellt,
und die manche Container-Runtimes, wie Docker, oder so,
bereitstellen.
Einen Container an sich gibt es in Linux so gar nicht.
Auch wenn das so genannt wird.
Das ist ein Sammelsurium aus verschiedenen Funktionen.
Und unter der Haube sind es ganz normale Prozesse, die im gleichen Kernel laufen,
wie alles andere, was ich sonst so ausführe.
Bei einer VM wäre das was anderes.
Wenn ich zum Beispiel jetzt sage, hier, wir machen mal eine Debian...
S...
Legt die mal neu an.
Das ist richtig.
Kubernetes ist keine Container-Runtime.
Kubernetes ist ein Orchestrierungs-Tool.
Ich weiß gar nicht, was Kubernetes mittlerweile standardmäßig für eine Runtime nimmt.
Container-D?
Oder Run...
Ich weiß gar nicht.
Ist das bei LXC gleich?
Richtig.
Das ist bei LXC genau das gleiche.
Seba...
Se...
Se...
Seba...
Su...
Chan...
95...
Se...
Se...
Se...
Se...
Se...
Se...
Se...
Se...
Se...
Se...
Chan...
95...
Danke für den Sub.
Das ist bei LXC genau das gleiche.
Diese ganzen Tools, ob Podman, ob Docker, ob...
Ja, was auch immer.
Aus irgendwelchen Gründen funktioniert meine Debian vor...
Oh, ich habe einen Kernel-Update gemacht, ohne neu zu starten.
Kann das sein?
Nice.
Starten meine VMs nicht.
Auch sehr cool.
Ja.
Pogu.
Geht ja gut los, was ich jetzt zeigen wollte.
Ja.
Vielleicht...
Vielleicht startet es jetzt?
Okay.
Scheint...
Jetzt...
Jetzt scheint es zu funktionieren.
Pogsbox, danke für den Sub.
Ja.
Das ist bei LXC genau das gleiche.
Das ist bei LXC genau das gleiche.
Also egal, ob es Docker, LXC oder Podman ist.
Das sind alles nur Frontends für die Linux-Kernel-Funktionalität, die man zusammenfasst als Container.
Ja.
Also Namespaces, C-Groups.
Natürlich gehört es auch...
Natürlich gehört es zu so einem Container, zu so einer Container-Runtime noch mehr.
Die muss das Image runterladen vom Container.
Die muss die Kernel-APIs ansteuern.
Aber unter der Haube ist das genau das gleiche.
Also es macht eigentlich keinen Unterschied, ob ich LXC, Docker oder Podman verwende.
Und am Ende ist es ein Prozess, der über die Kernel-Schnittstellen für, wenn es mal
Containerisierung gestartet wird.
Natürlich ist das Handling.
So.
So.
Und jetzt ist jetzt der Unterschied.
Also guckt euch mal an.
Uname-A, sagt er mir.
Es ist Arch.
Was sehr merkwürdig ist für den Ubuntu.
Warum das ein Arch-Kernel ist.
So.
Uname-A auf meinem Host und Uname-A im Container.
Ihr seht, es ist der gleiche Kernel.
Uname-A.
Uname-A in der VM ist Linux Debian Test-VM 5.10.
Also das ist der große Unterschied zwischen VMs und Container.
Auch wenn das immer ein bisschen über den Haufen geschmissen wird.
Eine VM ist wirklich ein Komplett-Virtualisierer.
Ein Komplett-Virtualisiertes System.
Inklusive eigenem Kernel.
Braucht dementsprechend auch mehr Ressourcen.
Was Speicherplatz angeht.
Was vor allem RAM-Usage angeht.
Deswegen sind Container deutlich schneller gestartet und leichtgewichtiger als VMs.
Weil Container gar nicht existieren.
Weil Container ganz normale Prozesse in meinem Host-Kernel sind.
Wie alles andere auch.
Nur ein bisschen besser gesandboxed.
Also das ist ein Unterschied.
Ob man was in der VM ausführt oder im Container.
Weil es ist grundlegend einfach was ganz anderes.
Das heißt, ich kann auch in der VM manche Sachen machen, die ich im Container nicht machen kann.
Ich kann im VM zum Beispiel Kernel-Module laden.
In einem Container kann ich keine Kernel-Module laden oder unloaden.
Mit speziellen Rechten mag das vielleicht gehen.
Aber zumindest teilen sich alle Container, die ich starte, die gleichen Host-Kernel-Module.
Ist ein Unterschied.
Der Sibaro war schon lange nicht mehr da.
Ist eine VM nicht auch ein Prozess?
Nee.
Eine VM ist zwar ein Prozess, der gestartet wird, um die VM zu starten.
Aber das ist wirklich dein komplett virtualisiertes System.
Du siehst ja beispielsweise auch nicht...
Okay, guck mal.
Wir machen jetzt nochmal einen Docker-Run.
Ähm...
Sleep 5.
So.
Und jetzt sage ich PSAux Grab Sleep.
Ich hoffe das funktioniert.
Da, guck mal da.
Was sehe ich denn hier?
Was sehe ich hier?
Als ganz normalen Prozess in meiner Prozess-Tabelle.
Was sehe ich hier?
Bash-C Sleep.
Was ein weiterer Beweis dafür ist, es ist ein stink-normaler Prozess, den ich im Kernel starte.
Äh, äh, den ich, den ich im Container starte.
Also ihr seht es ja...
Ups.
Jetzt habe ich, jetzt habe ich Mist gemacht.
So.
Hier.
Da steht es.
Laufende Prozesse.
Bin Bash-C Sleep 20.
Das ist das, was ich eigentlich im Container ausgeführt habe.
Also weiteres Beleg dafür, dass das stimmt, was ich gesagt habe.
Wenn ich was im Container starte, es gibt keinen Container.
Das ist ein ganz normaler Prozess, der hier in meinem System läuft.
Nur ein bisschen besser Sandboxed.
Und jetzt ist er weg.
Also das ist wie bei Portal, wo es den Cake nicht gibt.
Container gibt es eigentlich gar nicht.
Das ist ein Sammelsurium aus Sachen, was man so unter Container versteht.
Also es gibt keinen Container in dem Sinn.
Genau.
Und bei einer VM ist das anders.
Wenn ich eine VM starte.
Ich starte nochmal eine VM.
Eine Abstraktionsebene.
Ja, es ist ein bisschen, es ist eher so ein bisschen Sandboxed das Ganze.
Genau.
So.
Wenn ich eine VM starte und darin was ausführe, dann wird euch auffallen, da sehe ich nichts.
Okay, die VM läuft auch noch nicht.
VM läuft gleich.
Gut, dass wir den VM-Champ gebaut haben, oder?
Kann ich so Sachen besser zeigen.
Auf Debian.
Booten.
Please.
Break it.
So.
Und jetzt mache ich hier mal das Gleiche.
Was habe ich in der VM aus, was habe ich im Container ausgeführt?
Sleep 20 an A minus A.
So.
Und jetzt werdet ihr sehen.
Ich sehe nichts.
Was ich sehe, ist so Sachen wie zum Beispiel QEMU.
Dass das läuft.
Oder KVM.
Ne.
Ja.
Sowas sehe ich.
Also ich sehe, dass ein Prozess läuft, der eine VM virtuall.
Gestartet hat, wenn ich weiß, was dieser Prozess macht.
Aber ich sehe nicht, was in dieser VM läuft.
In der Prozessliste von meinem Host.
Weil es eben in einer komplett virtualisierten Kiste läuft.
Wo mein Host Kernel, mein Host System gar nichts mit zu tun hat.
Ja.
Das einzige, was ich hier starte, ist die Software, die die Virtualisierung macht.
Aber was dann in der virtualisierten VM läuft, hat mit meinem Host System nichts zu tun.
Dementsprechend sieht man auch hier, ich habe einen eigenen Kernel.
So wie ich das bei Debian erwarten würde.
Aber wenn ich jetzt exakt das Gleiche mache.
Guck mal Leute.
Ich habe hier oben gestartet.
In Debian 11 habe ich gestartet.
So.
Jetzt machen wir mal Docker run Debian.
Debian 11.
Ich weiß gar nicht, ob man 11 überhaupt machen kann.
Und ihr werdet feststellen, dass.
Ja ja.
Download Image.
Warte mal.
Ich mache das mal weg.
Und ihr werdet feststellen, dass der Unterschied schon ziemlich groß ist.
Hier ist es nämlich nach wie vor Host Kernel 6.2.11 Arch.
Und hier oben ist es das gleiche Debian System.
Es ist beides ein Debian System.
Es ist beides ein Debian 11.
Hier oben in der VM habe ich den Kernel 15 Debian.
Und hier unten habe ich, obwohl ich angeblich den Debian in Anführungsstrichen gestartet habe, nach wie vor 6.2.11 Arch.
Und man sieht, Container ist komplett geschwindelt.
Es gibt keine Container.
Es gibt nur paar Kernel Funktionalitäten und ein bisschen Kleber, die das zusammenführt als Container Runtime.
Und man nennt es dann halt Container.
So, aber genug zu dem Thema.
Das kann man nicht oft genug sagen.
Es ist egal, welche Container Runtime du benutzt.
Das ist im Prinzip immer das gleiche.
Man kann auch Container Images in einer VM ausführen.
Dann sind es trotzdem keine Container mehr danach.
Das sind VMs.
So, aber wo bin ich denn stehen geblieben?
Ich wollte eigentlich die zwei Features zeigen, warum man Terraform benutzt.
Also das erste habt ihr ja schon gesehen.
Man könnte jetzt reproduzierbar das in unterschiedlichen Projekten anlegen.
Egal, ob das jetzt in meinem Kekkel Stream 1 Projekt ist oder in Kekkel Stream 99 oder in Plup 1, 2, 3.
Egal, ich könnte reproduzierbar den ganzen gleichen Krempel anlegen.
Immer wieder mit Terraform auf die gleiche Art und Weise.
So, eine weitere coole Geschichte ist, Terraform merkt sich, was es gemacht hat.
Terraform hat einen State.
Terraform hat sich gemerkt, wenn ich jetzt hier nochmal Apply mache,
Terraform hat sich gemerkt, dass es Netzwerke für mich angelegt hat.
Das heißt, wenn ich jetzt nochmal Terraform Apply mache, schlägt es auch nicht vor, was zu machen,
sondern weil sie sagt, hier meine Infrastruktur und das, was ich konfiguriert habe, ist gleich.
Das heißt, Terraform weiß, dass es die Netzwerke angelegt hat.
Das bedeutet auch, wenn ich jetzt alles wieder löschen will, was ich mit Terraform aufgebaut habe,
dann könnte ich sagen Terraform Destroy.
Mache ich jetzt nicht, weil ich brauche sie ja gleich wieder.
Dann würde Terraform nachgucken und sagen, jawoll, ich werde einfach das, was ich angelegt habe,
nämlich das Network und das Subnet und hier so ein paar APIs eingeschaltet, werde ich einfach wieder löschen.
Und um euch zu zeigen, dass das auch tatsächlich der Fall ist,
oder wenn ich jetzt hier von Hand noch ein zusätzliches Netzwerk anlege,
hier bla bla plups, ist wurscht, wie das heißt.
Hier Subnet 122168.0.
Slash 4 und 2.
Moment, das ist der Name.
0w.
Region Asia ist vollkommen egal.
IP Range.
So, wenn ich das hier jetzt anlege und sage Terraform Destroy,
dann sagt Terraform nichts von diesem neu angelegten Netzwerk.
Warum nicht?
Weil Terraform es nicht selbst erstellt hat.
Terraform interessiert sich nur für Sachen, die Terraform selbst angelegt hat.
Ja, man kann Sachen importieren in Terraform, aber in der Regel ist es so,
Terraform hat es nicht selbst angelegt, also interessiert sich Terraform auch nicht dafür.
Terraform merkt sich, was es erstellt hat und kann das auch wieder löschen oder überprüfen,
ob das richtig ist.
So, jetzt gucken wir uns mal was weiteres an.
Ich löscht es, löscht es jetzt einfach wieder.
Ja, je nachdem was der macht, das ist nämlich das nächste, was ich euch zeigen will.
So, ich löscht den Krempel gerade mal wieder.
Delete.
Yes.
Also das nicht mit Terraform erstellte.
So, mal angenommen, ich gehe jetzt in das Netzwerk rein, was ich mit Terraform angelegt habe.
So, ich gehe jetzt in das Subnet hier rein.
Ich weiß gar nicht, ob man da was gescheit ändern kann.
Und ich sage jetzt, nee, nee, IP Range soll eigentlich Slash 25 sein, anstatt Slash 24.
Und ich speichere das jetzt.
Ja, natürlich, man kann immer noch viel zusätzlich machen.
Also, fuck ja.
Äh, 21.
Aber man darf es nicht kleiner machen.
Man darf es nur größer machen.
Dann fragt, oh warte, jetzt bin ich gespannt.
Jetzt bin ich gespannt, was Terraform macht.
Wahrscheinlich wird Terraform das Netzwerk neu erzeugen, weil es sich nicht ändern kann.
Oder es backt rum.
Gucken wir mal.
Bin ich mal gespannt, wie Terraform das jetzt händelt.
Weil Terraform müsste es ja eigentlich kleiner machen.
So, also irgendjemand ist jetzt ins Webinterface gegangen.
Und hat da dran rumgespielt.
Jetzt mache ich mal Terraform Apply.
Und jetzt guckt Terraform nach, wie ist der State, wie er sein soll.
Und wie ist der State, wie er wirklich ist.
Und jetzt sagt Terraform, Momente mal.
Guckt mal da.
Irgendwie ist es Slash 23, aber es sollte doch Slash 24 sein.
Terraform ist aber auch schlau genug, dass es weiß, es kann es nicht einfach ändern, sondern es muss replaced werden.
Dann kann ich jetzt sagen, jetzt sage ich Yes.
Und dann wird Terraform jetzt das Subnet löschen und das Subnet neu anlegen mit der richtigen Subnet Mask oder mit der richtigen Net Mask hinten dran.
Das sind die zwei richtig nicen Features an Terraform.
Dass Terraform weiß, was es gemacht hat.
Und Terraform Sachen korrigieren kann oder Sachen rückstandslos, rückstandslos, manchmal bleiben trotzdem ein paar Sachen übrig, rückstandslos wieder entfernen kann.
Neben dem reproduzierbar aufbauen.
Und das ist wirklich extrem nice.
Gerade wenn ihr im Projekt seid, also wenn mehrere Leute hier in diesem Projekt rumwursten.
Und ihr wollt danach nicht das komplette Projekt löschen, wenn ihr durch seid, sondern nur eure Ressourcen wieder entfernen.
Dann ist Terraform wirklich sehr praktisch.
Weil ansonsten müsstet ihr von Hand hingehen und alles im Web Interface wieder wegklicken, was ihr angelegt habt.
So und jetzt hat Terraform das wieder geändert.
Deswegen sage ich in Anführungsstrichen rückstandslos in Anführungsstrichen.
So und jetzt wird Terraform wieder sagen, jo passt.
Manchmal muss man mit Terraform Sachen exkluden.
Und ich sage das jetzt auch nur, weil manchmal das jemand im Chat gesagt hat.
Das kann man hier machen.
Man kann hier glaube ich ignore.
Man kann sagen, ey Terraform scheiß mal drauf, weil einer was von Hand geändert hat.
Aber das sollte man nur an Stellen machen, wo es auch wirklich notwendig ist.
Ansonsten versucht Terraform das immer in den Stand zu bringen, wie es hier im Code steht.
Was auch der Grund ist, warum sich das ganze Infrastructure as Code nennt.
Weil man seine Infrastruktur hier in Code, in Textdateien beschreibt.
Gut, jetzt machen wir mal was Sinnvolles.
Wir haben ja im Prinzip nur ein Netzwerk angelegt bisher.
Wir brauchen noch ein paar andere Sachen.
Und zwar, wir möchten ja VMs erzeugen können.
Ein Kubernetes Cluster erzeugen können.
In diesem Kubernetes Cluster.
Also mehrere Nodes für den Kubernetes Cluster erzeugen können.
Mehrere Pots laufen lassen können.
Und Deployments laufen lassen können im Kubernetes Cluster.
Das heißt, wir brauchen noch mehr IP Ranges.
Und das ist ein bisschen eklig gemacht.
In der Terraform Config für Google Cloud.
Wir machen Ignore Changes.
Ja, wir machen auch Ignore Changes bei einer VM gleich.
Nämlich für SSH Keys.
So.
Und dafür gibt es folgende Sachen.
Neben einer primären IP Range kann man für ein Subnet auch.
Es ergibt Netzwerktechnisch, Leute.
Ich sage es euch einmal an der Stelle.
Netzwerktechnisch ergibt das, was ich jetzt hier konfiguriere, keinen Sinn.
Aber man muss es bei der Google Cloud so machen.
Sonst funktioniert es nicht.
So.
Man kann jetzt nämlich noch sagen, mehrere Secondary IP Ranges anlegen.
Wollen einen Block haben.
Secondary IP Range.
Und da kann man jetzt noch zusätzliche IP Ranges definieren.
Die nicht zwangsläufig hier drinnen liegen müssen unbedingt.
Sondern andere.
Ja.
Da kann man zum Beispiel jetzt so etwas sagen wie Range Name.
So.
Also wir brauchen einmal eine Range für Kubernetes Pods.
Also letztendlich die Container, die ich in meinem Kubernetes Cluster laufen lasse.
Die sollen ja interne IPs kriegen.
Und das macht man über so eine Secondary Range.
So.
Da muss man wieder eine weitere Range angeben.
Und da können wir jetzt irgendwie so etwas machen wie hier 10.1.0.0.
Gut.
Dann können wir halt nur mal gucken, ob das geht oder ob es irgendwelche Auflagen gibt,
dass man das nicht machen darf.
Wir können auch Slash 16 machen.
Das ist vielleicht gar nicht so blöd.
Dann haben wir mehr Platz.
Dann haben wir mehr Pods.
Dann können wir quasi das hier für Pods benutzen.
So.
Das können wir machen.
Und dann brauchen wir nochmal was.
Das können wir jetzt keine Ahnung.
10.2.0.0.
Slash 16.
Oder wir können auch etwas kleineres nehmen.
Ist eigentlich vollkommen egal.
Das nächste ist für.
Wann brauchen wir das überhaupt?
Ich glaube.
Ich glaube, das reicht.
Für die für die Nodes selber.
Ich bin mir nicht sicher, ob wir das brauchen.
Müssen wir.
Müssen wir gleich machen.
Ja.
Ja.
Ja.
Ja.
Ja.
Ja.
Ja.
Ja.
Ja.
Ja.
Ja.
Ja.
Ja.
Ja.
Ja.
Ja.
Ja.
Ja.
Ja.
Ja.
Ja.
Ja.
Ja.
Ja.
Ja.
Ja.
Ja.
Ja.
Ja.
Ja.
Ja.
Ja.
Ja.
Ja.
Ja.
Ja.
Ja.
Ja.
Ja.
Ja.
Ja.
Ja.
dass wir das so machen, ansonsten
ansonsten funktioniert es nicht, so, machen wir mal
Terraform Apply, gucken, ob wir alles richtig gemacht haben
Wasche-Tabs
Die Wäsche-Tabs-Challenge
Gut
Weiter geht's
Terraform legt den ganzen
Darf das mehrfach
sein? Ja, das darf mehrfach sein
Wie gesagt, ich muss auch sagen, Terraform
ist zwar mit Abstand das verbreitetste Tool
dafür, ich persönlich mag die
Syntax von HCL nicht wirklich
Man muss sich halt ein bisschen mit auskennen
weil das, ja, weil das so zu den
Standard-Skills mittlerweile gehört
Ich persönlich mag die
Terraform-Syntax nicht, deswegen finde ich das
gut, dass sie sowas wie CDKTF
jetzt haben, dass man das mit normalen Programmiersprachen
machen kann, mein persönlicher Favorit
ist nach wie vor Pulumi
aber Terraform ist mit Abstand
das verbreitetste, schlicht und ergreifend
Diesmal wirklich, wieso?
Das ist das zweite Mal, wo wir das gemacht haben
Letztes Mal hatte ich keinen Bock mehr zu
streamen und gestern hatte ich Dünnschiss
Konnte ich nicht streamen
So, jetzt haben wir das angelegt
Ist die Playlist vorbei?
Ah, die war zu epic, die Playlist
Kann die Syntax komplex werden?
Ja, auch, aber die Syntax ist nicht schön
Zum Beispiel, wenn du Schleifen machen musst
und sowas, das ist einfach abartig
Es könnte ja durchaus sein, dass ich eine Schleife
machen will, ja, hierfür jetzt
für mehrere Netzwerke
Und Schleifen hier
Terraform Loops
Das ist nicht
wirklich aktuell
2016
Kein offizielles
Ding
Ja, dann machst du das so
Ja, dann schreibst du Count
und dann macht er das mehrfach
und dann kannst du hier so irgendwie
Count Index Variable einsetzen
Das ist wirklich abartig
Schleifen, Schleifen, Terraform
Das wird, da ist
Ja, genau, man kann dann auch
irgendwie so Wildcard Patterns
und so
komisches Gedöns machen
Das ist wirklich abartig
Also, insofern ist so Sachen wie
Pulumi schon ganz nice, weil man einfach
eine vollwertige, echte Programmiersprache hat
Alles ist gut
So, also, machen wir weiter
Das nächste ist äußerst unspektakulär
Das nennen wir
2
2 Router.tf
Und das kopiere ich jetzt, weil wir da schlicht und ergreifend
gar nichts machen müssen
So, ja
Das ist einfach
Äh, hups, vielleicht auch im richtigen
Ordner
So
Ich glaube, dazu muss ich jetzt nicht
Wollen wir nichts großartiges erzählen
Das nächste wird schon ein bisschen interessanter
Wir müssen, äh, NAT-Regeln anlegen
Also sprich, dass unsere
VMs zum Beispiel ins Internet connecten können
Ah
So, was ist mit dieser Playlist
los, Mensch?
Was bin ich denn? Du siehst Terraform-Config
Auf jeden Fall
So, als nächstes brauchen wir
NAT-Regeln
Damit unsere Kisten sich in die Internetseite verändern können.
So, das geht folgemaßen
Resource
Google
Compute
Router
NAT
Jetzt werdet ihr auch gleich sehen, warum wir einen Router anlegen mussten
Weil ich kann keine NAT-Regeln anlegen, ohne dass ich vorher einen Router angelegt habe
So
NAT-Regeln, damit unsere Kisten ins Internet kommen
Also, Name
NAT
Haben wir nur eins, so
Router gleich
Ähm, den hier im Endeffekt
Äh, Router
Moment, warte mal
Ich hoffe, das Ding kann Auto-Complete
Google Default Router
Genau, es kann Auto-Complete
Äh, Self-Link
Wenn man Self-Link benutzt, das ist auch so eine Google Terraform-Eigenheit
Dann bedeutet das, dass diese Ressource gelinkt wird mit dieser Ressource
Und die hängen voneinander ab
Das heißt, dann weiß Terraform
Das hier darf es erst nach dem hier machen
So, wenn ich das richtig verstanden habe
Ja
Ja
So, ähm
Jetzt müssen wir noch zwei andere Sachen anlegen
Und zwar müssen wir angeben, welche Region
Region haben wir eine Variable für
Unsere Region ist
So sieht man ja nicht
Unsere Region ist Europe-West-3
Also Region gleich Europe-West-3
Jetzt müssen wir ein bisschen Terraform-Matching machen für Google
Und zwar copy-paste ich mir das mal
Wir müssen folgendes angeben
Source-Subnetwork IP-Ranges to NAT
Da steht auch die Erklärung
Was das Ganze ist
High-IQ-Shit, also erlaubt ist
All subnetworks, all IP-Ranges, all subnets, all primary IPs and list of subnetworks
Das heißt, ich kann jetzt hier selbst angeben
Welche NAT-IPs das Ding verwenden soll
Beziehungsweise welche
Welche Subnets für NAT benutzt man?
Es ist Google-Logik
Wobei man das auch nicht zwangsläufig bräuchte
Man könnte es wahrscheinlich
Man könnte es auch
Ich lasse es mal weg
Vielleicht geht es auch so
Mal ausprobieren
Das habe ich tatsächlich auch aus einem Tutorial
Also weiß nicht, warum man das so macht
Das ist von Cloud-zu-Cloud-Anbieter ein bisschen unterschiedlich
Zumindest muss man jetzt noch die NAT-IPs angeben
Also sprich, welche ausgehenden IP-Adressen man verwendet
Das heißt, man muss die IPs angeben
Welche ausgehenden IP-Adressen man verwendet
Und wer jetzt denkt, man könnte hier einfach sowas reinschreiben
Naja, ist ein bisschen zu einfach
Weil ich weiß ja meine IP nicht
Es sei denn, ich habe die im Vorfeld angelegt
Das heißt, ich muss jetzt auch externe IPs anlegen
Und das geht mit Google Compute Address
So, da kann man NAT-IPs oder irgendwie sowas angeben
Übrigens, das sollten wir auch mal NAT nennen
Und da kann man dann sagen
Hier Name
Address-Type
Das ist jetzt wichtig
Address-Type muss man reinschreiben
External
Und Network-Tier können wir Standard lassen
Depends on
Brauchen wir auch nicht
So, das müsste jetzt eigentlich so funktionieren
Und an der Stelle sage ich
IPs benutzt doch mal bitteschön die IPs von Google Compute Address Punkt NAT
Da geht das AutoComplete wieder nicht
Wo AutoComplete geht?
Und wann AutoComplete geht?
Keine Ahnung
Ich glaube das Terraform Plugin von Visual Studio Code ist einfach eklig
Genau, ich hoffe das funktioniert so
Es könnte sein, dass ich den Subnet angeben muss
Achso, ich habe eine List of Subnets angegeben
Ich will ja nicht
Ich will...
Was ist hier möglich?
Was kann man hier angeben?
Source Subnet
Moment
Source Subnetwork IP ranges to NAT
How NAT should be configured per Subnet
If all subnets of all of the in every are allowed
Ja, das ist durchaus okay
If all primary IP ranges
Wollen...
Ne, wir wollen all IP ranges haben
Ne, wollen wir nicht
Wollen wir nicht
Wir wollen nur NAT erlauben aus diesen IP ranges
Wisst ihr was?
Scheiß drauf
Wir tragen jetzt mal da alle ein
So
Ich glaube, dass das Security-Technik nicht die beste Variante ist
Und übrigens, man könnte glaube ich hier an der Stelle auch Auto machen
Probieren wir das mal aus
Weil dann brauche ich wahrscheinlich diese Einträge hier nicht
Schauen wir mal
Mal gucken, ob sich das applyen lässt
Ich bin mir da selbst nicht ganz sicher
Ich habe es ja im Vorfeld schon ausprobiert
Aber im Vorfeld habe ich es anders gemacht
Aber ein bisschen die Sachen vereinfachen ist ja nicht verkehrt
So
Also, er will jetzt ein paar Regeln anlegen
Und ein NAT-Objekt und ein Router und so
Mal gucken, ob er Fehler schmeißt
Wenn er Fehler schmeißt, dann habe ich es verkehrt gemacht
Zumindest kommen wir jetzt gleich zu einem interessanten Part
Wir sind ja noch bei Netzwerk-Config
Das ist relativ...
Ah, ja, okay
That's all we know
Nice, GG Eats
Da weiß man auch sofort Bescheid, oder?
Es ist komplett, komplett obvious, was kaputt ist
Also, wenn ihr nicht versteht, dann cringe, ja
Es ist eigentlich logisch, woran es liegt, oder?
Versteht man, versteht man doch sofort
Nicht eingeloggt, doch, doch, ich bin eingeloggt
Ich habe irgendwo einen Config-Fehler drin
Äh, ja
Also, ich kann jetzt einfach mal das kopieren, von dem ich weiß, dass es geht
Ähm
Wie habe ich das Netzwerk genannt hier? Internal... ähm...
Also, ich hätte jetzt...
Ich habe es ein bisschen vereinfacht jetzt für den Stream
Ich habe jetzt eigentlich gehofft, das funktioniert auch
Was er für Probleme hat, man weiß es nicht
Router was not found on the server
Warum?
Habe ich hier vielleicht... Moment, Router?
Muss ich... Habe ich das falsch...
Ah, muss man Router nicht self-link, sondern name machen? Vielleicht
Yes
Ja, es ist teilweise etwas undurchsichtig, was man braucht
The name of the Cloud Router in which that will be... Okay
Hier muss man den name angeben, es steht ja sogar da
Es steht ja sogar da
Leider ist es halt das dumme, dass es nicht Router name heißt, weil dann wüsste man gleich, was Sache ist
Aber so ist das halt
So, jetzt hat es funktioniert
Nice
Gut
Kommen wir jetzt mal zu ein bisschen was Spannenderem
Ähm, nämlich... Na gut, wir müssen noch eine Firewall-Regel anlegen
Das ist tatsächlich jetzt nicht so spannend
Machen wir ein neues File
4... oder Firewall-Rules
.tf
Imagine... Ja, die Variablen sind wirklich nicht so nice
So, das copy-paste ich, weil das ist relativ selbsterklärend
Also, wir wollen SSH-Zugang erlauben, und zwar von überall
Von überall wird SSH erlaubt auf Port 22
Nachdem, um das jetzt mal ein bisschen zu relativieren, dass das Security technisch gar kein so großes Problem ist
Es ist nichts davon public im Internet verfügbar
Also ihr müsst euch vorher authentifizieren, bevor ihr euch überhaupt versuchen könnt, über SSH einzuloggen
Was im Endeffekt heißt, es ist nicht schlimm, dass wir von allen Ranges das Ganze erlauben
Ja, also...
SSH ist überall erlaubt
Und jetzt kommt tatsächlich, finde ich, was ganz interessantes
Und danach führen wir es auch nochmal aus
Wir brauchen eine Jump-VM
Denn bisher ist alles private in der Google Cloud, was wir angelegt haben
Wir haben ja private Netze
Der private Kubernetes-Cluster wird auch nicht von außen zugänglich sein
Das heißt, wir müssen jetzt irgendwas anlegen
Und zwar eine, nennen wir es mal, Jump-VM.tf
Das ist eine virtuelle Maschine über Google Cloud Compute
Eine VM-Instance
Die legen wir jetzt nicht im Webinterface
Einmal hier ganz normale Google Cloud VM
Die Dinger sind übrigens schweineteuer
Ich zeige euch nochmal exemplarisch
Also wenn ihr einfach nur eine VM wollt, ist Google Cloud der Witz
So eine popelige VM, 28 Dollar
Und dann gucken wir mal hier, Europe
Europe, Frankfurt
So eine popelige VM
Zwei CPUs
Und das ist noch die Shared-Plattform
Machen wir mal was ordentliches
N2 Standards
Die wollen ernsthaft 74 Euro
Für eine 2,8 GB VM
Da kriegst du bei Hetzner 3 Server für
Mit ordentlich Power
Kann man die 1 überhaupt noch auswählen?
Das sind die alten
Gibt es die 1 noch?
Ja, First Generation
Die sind ein bisschen billiger
Aber auch nicht, aber auch nicht so viel
Guck mal, 32 Euro für
Einen Kern
Und 4 GB RAM
32 Euro
Jetzt guck mal, was wir, pass mal auf
Jetzt guck mal, was wir bei Hetzner Cloud dafür bekommen
Prices
So, bei Hetzner Cloud kriegen wir, wir brauchen
Ja, komm, mit IP
Das macht man aber ohne
So
Ok, Dedicated machen wir auch
Also
Wir kriegen, bei Hetzner würden wir tatsächlich
Dedicated
Auch eine
Wie viel, wie viel kostet's?
32 Euro
Ja
Da würden wir 2 CPUs und 8 GB RAM kriegen
Versus eine CPU und 4 GB RAM
Besser bei Hetzner würde ich dann tatsächlich sogar
Die hier nehmen
Und da kriegen wir dann tatsächlich
8 CPUs
Und 16 GB RAM
So
Aber gut, das ist, das ist vielleicht fairer
Wenn man es dann mit denen vergleicht
Die sind nämlich auch Shared
Shared Medium
Also 2, 2 V-CPUs, 4 GB RAM
32 Euro, auch nicht viel billiger
32 Euro
Ja, also, man kriegt bei Google, kriegt man
2 CPUs, 4 GB Memory
Und bei Hetzner kriegt man quasi
8 CPUs und 16 GB RAM
Wo ist der Unterschied?
Shared und Not Shared
Ja, ob es exklusiv für dich ist
Oder ob andere auch noch Zeit
CPU-Zeit abbekommen
Also das können dann beispielsweise
Das könnte dann zum Beispiel
Ein Vm-Cluster sein
Wo halt
Nicht nur
Du drauf bist
Sondern auch ein paar andere Leute
Shared kann manchmal ein bisschen langsamer sein
Ja
Bei, bei, hier ist es so
Du kannst, du kannst
Wenn du kurzzeitig
Wenn du kurzzeitig mehr brauchst
Dann
Es kann aber auch sein
Dass ich jetzt sogar Mist erzähle
Wie es ist
Wenn du
Oder halt, dass ich Mist erzähle
Aber, aber ich, ich glaube
Es ist, wenn du
Wenn du kurzzeitig mehr brauchst
Dann kriegst du für, glaube ich
Bis zu 120 Sekunden
Auch ein bisschen mehr Leistung
So, das, das, das ja
Also was ich, was ich euch zeigen wollte
Vm's sind unsinnig teuer
Über die Google Cloud
Also einfach nur Vm's anlegen
Würde ich da nicht
Würde ich da nicht machen
So
Also wir erstellen jetzt mal eine Vm
Resource
Google
Compute
Nicht Computer
Google Compute Instance
Nennt sich das ganze
Instance
Name heißt
Äh
Jump Host oder sowas
Jump
Name
Hallo
What the
Fuck
Name
Alles klar
Name gibt es nicht
Sehr gut
Obwohl, obwohl ich weiß
Dass es Name gibt
An der Stelle
Das Auto komplett ist
Pega und Brocken
Äh
Was auch immer
So
Jetzt muss ich hier
Maschinen Type angeben
Maschinen Type ist das
Was ich hier auswählen kann
Und wir nehmen die kleinsten
Dies gibt
Als Jump Host
Also was
Was im Monat
Nur noch
8 Euro kostet
Also wir wollen haben
Etu
Mikro
Übrigens
Ich bin der Meinung
Die könnten das Auto
Komplett viel viel besser machen
In dem
In der Terraform
Visual Studio Code
In der Internet
In der Internet
In der Internet
Visual Studio Code Extension.
Weil du kannst zum Beispiel hier
nicht Auto-Compliten.
Das wäre ja ohne Probleme möglich,
was einzubauen, was du scannst
für den Google-Provider.
So, MachineType E2Micro,
das ist die billigste 4M, die es gibt.
Dann müssen wir noch eine Zone angeben,
wo das Ganze erstellt werden soll.
Zone ist bei uns Frankfurt.
Also,
Europe West 3
minus A ist unsere Zone.
Wir müssen
eine Sache noch customizen, nämlich die
Boot-Disk.
Und zwar
das Image.
Warum kann ich nicht Enter drücken an der Stelle?
Das Image ist
Debian Cloud
slash Debian 11.
Also das, was wir eben auch
nach übrigens Auto-Compliten
wieder
komplett procken, das automatisch formatieren.
Wir wollen ein Debian 11
drauf machen.
Und wir brauchen ein Network-Interface,
ein
Network-Interface für die 4M.
Da gehen wir an Network
und an der Stelle im Prinzip
unser Default-Network,
was wir vorhin angelegt haben.
Und die 4M soll seine IPs beziehen
von
vom internen Subnet.
Also von
10.0.0.
10.0.0.0
slash 24.
Das ist ja das, was wir
konnte man hier nicht eigentlich
auch, nee Moment, ich hab was vergessen.
Self-Link muss ich noch machen.
Ja, hier, da, da kommt das her.
So.
Das war's. Eine Sache fügen wir aber noch ein.
Sonst wird's nämlich eklig.
Lifecycle ignore changes.
Man kann Terraform sagen,
dass es manche Änderungen,
die außerhalb von Terraform passieren,
ignorieren soll.
Und ich finde es sinnvoll,
wenn man die SSH-Keys nicht über Terraform
selbst managt,
diese zu ignorieren, weil, wenn ich mich
sonst dort per SSH einlogge,
und dann meckert Terraform das nächste
Mal rum und sagt, oh, die SSH-Keys
sind unterschiedlich, ah, ich möchte die
4M jetzt bitte anpassen. Wollen wir nicht.
SSH-Keys kann Terraform ignorieren.
Aber die 4M soll sie tracken und sollen
sie anlegen. So, und wenn ich das
jetzt richtig gemacht habe,
alles, dann
apply,
dann sollte Terraform jetzt vorschlagen,
eine 4M anzulegen.
Und
ja, sieht gut aus.
Network Interface,
Bootdisk, neue Compute Resource
und neue Firewall-Regeln
für SSH, alles klar. Ja, Terraform
legt es mal an. Und wenn ich alles richtig gemacht
habe, dann hat
Terraform gleich eine 4M
erzeugt für mich.
Da ist sie doch schon.
Ja, Internal IP
1002. Wir können auch
gleich mal ausprobieren, ob wir da drauf kommen.
Können wir ausprobieren.
Sagen wir nämlich jetzt einfach Cloud Compute
SSH, und zwar zur
Jump, zu Jump.
Und jetzt sagen wir Tunnel Through IAP.
Braucht man, macht er eigentlich
automatisch, braucht man aber dann,
wenn die 4M selber
keine Public IP hat.
Ja, direktes SSH
auf die 4M geht nur,
wenn die 4M selbst eine Public IP hat.
Hat sie nicht, sie hat nur eine Interne IP.
Deswegen muss man das durch diesen,
muss man sich SSH tunneln durch so einen
extra Google Tunnel. So, mache ich
das jetzt mal.
Und jetzt generiert er mir
erstmal SSH-Keys und
überträgt diese SSH-Keys
an die 4M.
Deswegen habe ich auch, jetzt ergibt
das auch Sinn, deswegen habe ich eben auch
in Terraform gesagt, Terraform, ignorier mal bitte
die SSH-Keys, sonst würde beim nächsten
Apply Terraform jetzt sagen,
oh nein, die SSH-Keys haben
sich geändert. Jetzt ist ganz böse
und das muss jetzt unbedingt
überschrieben werden.
Und ihr seht, ich bin auf meiner Cloud 4M
drauf. Funktioniert auch.
Update. Mein NAT
funktioniert. NAT ist richtig eingerichtet.
Schau mal, sehr gut, hat funktioniert.
Jo.
Jetzt habe ich eine 4M. Über die 4M
kann ich mich jetzt in meine Google Cloud
verbinden, weil ihr seht ja,
Google Cloud technisch ist
alles nicht aus dem
Internet erreichbar. Das kann ich jetzt
nur nochmal betonen.
Da kommt man ja easy drauf. Ne, ne, man kommt nicht easy
drauf. Das ist alles ein internes
Netz, Private IP
Range und nicht aus dem Internet
erreichbar.
Das heißt, auch wenn ich hier irgendwelche Security,
problematische Security Sachen mache,
ist jetzt erstmal nicht so schlimm, weil die
Sachen sind aus dem Internet eh nicht erreichbar.
Was nicht heißt, dass ich nicht bestimmte
Services, ähm, ja,
erreichbar machen
könnte. Okay.
Weiter geht's.
Wir haben jetzt alles Wichtige angelegt.
Also wir haben unsere Jump4Ms,
wir haben unsere
Netzwerke. Jetzt kommen
wir mal zum Kubernetes Cluster.
Und um den Kubernetes Cluster
anzulegen, wir machen eine Sache nochmal zwischen
durch. Da bin ich mir nicht ganz so sicher,
warum man das braucht, aber in dem
Tutorial stand das drin. Das heißt, ich
vermute mal, das hat irgendeinen
irgendeinen Sinn.
Wie ich es immer im falschen Verzeichnis anlege.
Und zwar, wir legen noch
ein Service Account an für unser
Kubernetes. Ich vermute
mal, das ist später relevant,
wenn ich in meinem Kubernetes Cluster
auf Secrets zugreifen will und sowas.
Oder um
vielleicht von anderen Services
auf meinen Kubernetes Cluster zuzugreifen.
Ich bin mir nicht ganz sicher, wozu man das braucht.
Ich habe es bis jetzt noch nicht gebraucht,
aber ich vermute mal,
es hat seinen Sinn, dass sie das im Tutorial auch
gemacht haben.
Und jetzt kommen wir zu unserem Kubernetes
Cluster. Ich Terraform applye
nochmal, dass wir hier up to date
sind.
Ja, yes,
alles gut, das wird klappen. So,
jetzt kommen wir zur eigentlichen Sache, die wir machen wollen,
nämlich unser Kubernetes Cluster anlegen
in der Google Cloud. Dazu
legen wir auch wieder Terraform an. Terraform
File an 7 minus
Kubernetes Cluster
.tf. Und das ist
jetzt in der Tat etwas trickreicher,
wie das funktioniert mit einem Kubernetes Cluster.
Und zwar, das erste, was man machen muss, natürlich,
man muss eine Ressource anlegen.
Google Container
nennt sich das, Cluster.
Nicht Kubernetes, nein, Google
Container Cluster.
Dem kann man
einen Namen geben und in dem Tutorial, was ich geguckt
habe, haben sie ihn Primary
genannt, was ich eigentlich ganz sinnvoll finde, weil
das der einzige Kubernetes Cluster ist, den wir anlegen. Man könnte ihn auch Cluster oder wie
auch immer nennen. So, name, gleiche wie gehabt. Location und da ist es jetzt wichtig, dass wir
sagen location.zone. Ansonsten braucht er nämlich viel zu viel Disk Space. Wenn ich hier sagen würde
location.region, dann würde er, also wenn ich jetzt region angebe anstatt zone, dann würde das
replizieren dreimal alles. Und dann bin ich an meinem Storage Quota. Also wir haben nur einen
Single Location Cluster. Ist nicht sonderlich hoch verfügbar. Gut, aber für die Demo jetzt egal.
Nächste Sache, die auch wichtig ist, removeDefaultNodePool. Das müssen wir auf true setzen.
Und true 0w und den InitialNodeCount. Alter, was ist denn mit dem? Warum ist diese
Echtzeit? Warum ist diese Echtzeit? Warum ist diese Echtzeit? Warum ist diese Echtzeit? Warum ist
diese Echtzeit? Warum ist diese Echtzeit? Warum ist diese Echtzeit? Warum ist diese
Extension so kacke? Auf 1. Was das macht, ist normalerweise, wenn ein Google
Kubernetes Cluster, Google Kubernetes Engine Cluster erzeugt wird, bringt er einen Standard
NodePool mit. Das wollen wir nicht. Wir wollen einen extra NodePool anlegen, weil es könnte
ja sein, dass wir unterschiedliche Workloads für den Cluster haben. Dass wir zum Beispiel
Workloads haben, die besonders starke Kubernetes Nodes brauchen und welche
die mit weniger starken Kubernetes Nodes auskommen. Das spart Kosten. Also ich kann natürlich überall
die fettesten Kubernetes Nodes hinstellen, aber das kostet halt. Wenn ich sage, hey,
wir machen einfach NodePools, einmal ein NodePool mit BilligNodes und einmal ein NodePool mit
HighPerformanceNodes, dann kann ich mir Geld sparen. Ich kann natürlich auch alles mit
HighPerformanceNodes aufbauen, aber es ist wahrscheinlich gar nicht notwendig. Das heißt,
wir löschen den HighPerformanceNodeCount. Das heißt, wir löschen den HighPerformanceNodeCount.
Das heißt, wir löschen den HighPerformanceNodeCount. Das heißt, wir löschen den
initial angelegten Pool für Nodes in diesem Cluster und legen gleich selbst von Hand noch
einen NodePool an. Nodes sind bei Kubernetes im Prinzip ja die Teile, aus denen sich der
Kubernetes Cluster zusammensetzt. Ein Kubernetes Cluster besteht aus mehreren Nodes. Auf jedem
Node läuft dann das ganze Kubernetes Zeug immer. Also nicht die Container mit der Anwendung,
sondern das ganze Kubernetes Management Zeug läuft auf jedem Node.
So, also erstmal alles weg. Jetzt müssen wir dem Ding natürlich noch sagen, welches Netzwerk soll
das denn benutzen und da wollen wir unser Default Network, falls es hier irgendwo steht. Network
Default. Dann muss man dem Ding noch sagen, welches Subnetwork es benutzen soll und das
soll unser Default Subnetwork benutzen. Logging, Monitoring lassen wir aus. Es gibt noch eine
wichtige Sache, Networking Mode und da sollten wir reinschreiben. Wir können ja mal gucken, was alles.
Ah super, gibt keine Hilfe, gibt keine Hilfe. Wir wollen VPC Native haben. Es gibt die Möglichkeit,
dass die IPs geroutet werden aus dem Kubernetes Cluster oder das IP-Aliasing benutzt wird.
IP-Aliasing ist grob gesagt auf einem Netzwerk Interface mehrere IPs und Routing ist Routing.
Anderes Netz über den Gateway und IP-Aliasing ist
glaube ich schneller und einfacher. Jetzt können wir noch ein paar andere Sachen machen. Wir könnten
zum Beispiel noch den Standard Ingress Controller löschen und sowas. Das lassen wir uns, sparen wir
uns jetzt aber alles, weil so viel Kubernetes Config machen wir heute nicht. Wir legen den
Cluster an, reproduzierbar. So, jetzt ist noch eine weitere wichtige Sache. Wir müssen ihm sagen,
was für Kubernetes soll denn da eigentlich drauf? Und es gibt die Auswahl zwischen Stable, Regular
und
Rapid, genau. So, da kann man mehrere, da gibt es verschiedene Release Cycles. Wenn ich sage Stable,
gibt es alle paar Monate Upgrades. Wenn ich sage Regular, gibt es mehrfach im Monat Upgrades und
wenn ich Rapid sage, kriege ich Kubernetes Upgrades wöchentlich. Also, wenn ich immer eine super
aktuelle Kubernetes Version haben möchte, dann nehme ich Rapid, ansonsten nehme ich Regular.
Wir nehmen jetzt Regular.
Regular.
Das ist tatsächlich eine wichtige Geschichte und ich habe gesehen, bei uns auf der Arbeit,
habe ich schon einen Fehler entdeckt oder ich vermute, dass es ein Fehler ist. Ich habe gesehen,
dass die irgendwo hardcoded in ihr Terraform Kubernetes Versionen reingeschrieben haben.
Das wird über kurz oder lang zu Problemen führen, weil die Cluster geupdatet werden und Terraform
dann eine andere Version sieht, als bei ihm im State steht. Also, das wird, es wird noch Probleme geben.
Muss ich ja gleich mal morgen gucken, ob das wirklich so ist oder ob das nur für mich so ausgesehen hat.
Rapid ist aber nicht Unstable. So wie ich das verstehe nicht. Ich gehe davon aus, es sind nur
normale Kubernetes Releases, aber halt schnell, nachdem sie rausgekommen sind. Und da buggt ja
auch ganz gerne mal was. Also, wenn du nicht neue Features brauchst, kannst du durchaus auch auf
Stable gehen. Also, es sind keine Beta Releases, sondern es geht einzig und allein, wie oft wird geupdatet.
Also, es sind keine Beta Releases, sondern es geht einzig und allein, wie oft wird geupdatet. Also, es sind keine Beta Releases, sondern es geht einzig und allein, wie oft wird geupdatet.
Also, es sind keine Beta Releases, sondern es geht einzig und allein, wie oft wird geupdatet.
Und Kubernetes Versionen kommen ja relativ oft raus. Wir können sonst, erinnere mich mal dran, wir können im Web Interface später gucken, was zur Auswahl steht.
Oder wir können auch so bei Google gucken.
So, also wir gehen mit Regular, ganz normal, ganz normale. Update Intervall wollen wir haben.
So, und jetzt muss man sagen, jetzt wird es super weird champig, was man einstellen muss.
Man muss eine IP Allocation Policy einstellen. Das ist, aus welchen IP Ranges,
Kubernetes, also die Services auf dem Kubernetes Cluster selbst, IPs beziehen sollen.
Und aus welcher IP Range die Pods, also die Container, die auf dem Kubernetes Cluster laufen, ihre IPs beziehen sollen.
Und das bekommen wir hier her. Guck mal, bei unserem Netzwerk haben wir doch das hier angelegt.
K8S Services, K8S Pods. Das brauchen wir an der Stelle jetzt. Also, IP Allocation Policy.
Man kann, ach guck mal, man kann das auch so direkt zuordnen. Das ist vielleicht auch nicht schlecht.
Ah, nice. Also man kann hier sagen, okay, Services, Cluster Secondary IP Range Name.
Das ist das, was wir dort angegeben haben. Cluster Secondary IP Name.
Und entweder können wir jetzt da reinschreiben, K8S Pods.
Und K8S Services, das ist übrigens Secondary Services, Secondary Range Name.
Dass er weiß, wo er seine IPs herbeziehen soll.
Ich vermute mal, man kann hier auch das irgendwie linken, indem man sagt, das muss ich jetzt mal ausprobieren, indem man sagt,
Network, nee, Google Subnets Internal Secondary IP Range.
Google Subnets Internal Secondary IP Range.
Google Subnets Internal Secondary IP Range.
Kann man da mehrere angeben? Hier, Secondary IP Range Name.
Nee.
Okay, da bräuchte ich jetzt irgendeinen Terraform-Experten, der weiß, wie das funktioniert.
Ich weiß nicht, ob man da sowas angeben kann.
Aber wenn man den Namen braucht, ist der eh blöd.
Na gut.
Wir schreiben es so rein, da weiß ich, dass es funktioniert.
So, also, das ist das Wichtigste hier.
Die Container, die ich auf dem komponiertes Cluster starte, die ziehen ihre IPs
aus dieser IP Range.
Intern.
Also, da bekommen die eine zugewiesen, wenn sie gestartet sind.
Beziehungsweise, wenn sie gestartet werden.
Das Wichtigste, das muss man machen.
Und jetzt noch eine wichtige Config für alle Cluster, die nicht öffentlich im Internet verfügbar sein sollen.
Man muss als erstes einstellen, Enable Private Nodes.
Und dann muss man einstellen, Enable...
Geist, Alter.
Enable Private Endpoint.
Die zwei Sachen muss man machen.
Und wenn man das gemacht hat, noch mal weirdchampiger, muss man noch mal eine extra IP Range angeben.
Für die Managed Kubernetes Cluster Seite von Google.
Und das nennt sich Master IPv4 Sidre Block.
Weil, fragt mich nicht.
Und dann nehme ich das, was Google in seinem Beispiel hingeschrieben hat.
Das hat nichts mit meinen IPs zu tun.
Das ist die IP Range.
Das ist die Managed Kubernetes Service von Google.
Was die dafür nehmen soll.
Das ist nicht, worum ich mich kümmern muss.
Aber ich kann es trotzdem aussuchen.
Weil ich mich vielleicht zu irgendeiner API oder so connecten will.
Aus diesem Bereich.
Das ist übrigens die offizielle Range, die Google vorschlägt, dass man die dafür nimmt.
Es muss Slash 28 sein, was anderes ist nicht erlaubt.
Genau, und am Ende kopiere ich mir noch was raus.
Am Ende gibt es noch eine Config.
Welche internen Netze alles auf meinen Kubernetes Cluster zugreifen dürfen.
So.
Who cares?
Wurscht.
Next.
So.
Wir haben den Default Pool gelöscht.
Da.
Guckt hier.
Remove Default Node Pool.
Wobei.
Wir können den Kubernetes Cluster jetzt eigentlich erstmal anlegen lassen.
Apply.
Ich hoffe das geht ohne Pool.
Und ich warne euch jetzt schon mal, das kann durchaus 4-5 Minuten dauern.
Das ist mega langsam Kubernetes Cluster zu professionieren in Google Cloud.
Allerdings in allen anderen Cloud Plattformen ist es ähnlich langsam, wenn nicht sogar noch
langsamer.
Wir checken mal, ob wir alles richtig gemacht haben.
Nein.
Haben wir nicht.
Oh ne.
So.
This is secondary IP Range.
Ja.
Kaum macht man es richtig.
Ja.
Kaum macht man es richtig, dann wird es auch funktionieren.
Sag mal, ich habe es früher falsch gemacht.
Wie beschissen ist diese Visual Studio Code Extension eigentlich?
Jetzt weiß ich auch, warum die nur 2,5 Sterne von 5 hat.
Oder nur 2 von 5 Sterne.
Weil die einfach suckt.
Ich weiß nicht.
Ich weiß nicht.
Ich weiß nicht.
Ich weiß nicht.
Ich weiß nicht.
Ich weiß nicht.
Ich weiß nicht.
Ich weiß nicht.
Ich weiß nicht.
Ich weiß nicht.
Ich weiß nicht.
Ich weiß nicht.
Ich weiß nicht.
Ich weiß nicht.
Oh, was habe ich verkehrt gemacht?
Null.
Ok, Primary Network Default Cluster.
Ah, das habe ich falsch gemacht.
Ah ja, Self-Link, weil Google Logik Self-Link.
Das sieht schon besser aus.
So, und dann jetzt Glück haben.
Schlägt der mir vor, ein Kubernetes Cluster anzulegen?
Hier war der End Point, so.
Yes, und wie gesagt, Warnung, das nicht wundern.
Es dauert 4-5 Minuten oftmals, bis das Ganze erstellt ist.
Also nicht wundern, nicht zwischendurch abbrechen, das dauert einfach eine Weile.
Man kann dem ganzen Treiben im Webinterface zugucken, zumindest nach einer gewissen Weile.
Wenn wir in die Google Kubernetes Engine gehen, sieht man, okay, der Cluster ist schon da,
aber er ist noch am Professionieren und Node Numbers 1 stimmt auch nicht.
Das sind die initialen Nodes, die dann wieder gelöscht werden.
Bei der Gelegenheit würde ich euch empfehlen, werft mal einen Blick auf...
Alter, ich habe keine Push-Notification gekriegt.
Ich soll doch bitte Firmware-Update auf meinem Katzenbrunnen machen.
Dafür kriege ich Push-Notifications. Alter, geht mir nicht auf den Sack.
Das wird gleich ignored.
Und jetzt kommt noch der Weekly Cleaning Reminder.
Na gut, jetzt habe ich ganz vergessen, was ich sagen wollte, Leute.
Ja, das kann ein bisschen dauern, bis der Kubernetes Cluster angelegt ist.
Ja, aber wir sollten mal auf Billing gehen.
Und wir gucken uns die Quotas an.
Und da wird man gleich sehen, das dauert ein bisschen, bis das aktualisiert ist.
Und da wird man gleich sehen, dass der Speicherplatz verbraucht.
Das dauert manchmal ein bisschen, bis die Quotas aktualisiert sind.
Gehen wir mal hier, Total SSD.
Ist das das Richtige?
Ne, das ist nicht das Falsche.
SSD.
Hier, SSD.
SSD Total Storage.
Ja, da habe ich ein Limit von 250 Gig.
Und mein Kubernetes Cluster wird ein bisschen was brauchen.
Das braucht aber ein bisschen, bis das aktualisiert ist.
Manchmal nicht so schnell.
So.
Irgendwann sollte der Kubernetes Cluster fertig angelegt sein.
Wie gesagt, das dauert ein bisschen.
So.
Und irgendwann aktualisiert jetzt auch das Quota.
Ja.
Und dann wird er wieder angezeigt, dass er irgendwie 100 Gig oder so verbraucht hat.
Oder was auch immer.
Ne, Moment.
Der sollte mir gar nichts anzeigen, weil ich ja alle Nodes wieder lösche.
Null.
Ne, aktuell ist es sogar gut, dass da Null steht.
Ja.
Weil der löscht ja wieder alle Nodes.
Da dürfte nur, wenn überhaupt, kurz zwischenzeitlich mal 100 stehen.
Guck hier, jetzt steht 100 da.
Current Usage 100.
Weil standardmäßig ein Kubernetes Cluster, wenn man nichts explizit angibt,
ein Node 100 Gigabyte SSD speichert.
Der sollte der aber auch gleich wieder löschen, weil ich habe ihm ja gesagt,
hier, Delete, also Remove Default Node Pool.
So.
Während er anlegt, wir können ja eigentlich schon mal weitermachen
und wir können unseren Node Pool anlegen.
Nennen wir mal K8S Node Pool, Node Pool Punkt PF.
So.
Und da stellen wir jetzt ein, die ganzen Nodes von unserem Cluster.
Wie gesagt, ein Kubernetes Cluster besteht aus unterschiedlich vielen.
Die Nodes müssen nicht zwangsläufig alle die gleiche Hardware sein.
Das macht man ganz gerne mal, vor allem, wenn man das selbst aufbaut.
Aber ein Kubernetes Cluster kann aus unterschiedlichen Nodes bestehen.
Was eventuell auch ganz sinnvoll ist, wenn ihr zum Beispiel ein paar Anwendungen habt,
die viel Performance brauchen und ein paar sporadische Tasten und ein paar Low Performance Anwendungen,
wo man zum Beispiel auch damit leben kann, dass...
die erstmal eine Weile brauchen, bis sie starten und sowas in der Richtung.
Da kann man mehrere Node Pools machen.
Da muss man aber halt sagen, worauf was professioniert werden soll.
Das heißt, wir legen heute nur einen Default Node Pool an.
Und ich behaupte mal, das ist standardmäßig auch zum größten Teil so.
Ja, dass es ein Cluster gibt und ein Node Pool gibt.
Aber es ist tatsächlich ganz praktisch, dass ich den Node Pool unabhängig einstellen kann vom Cluster.
Nämlich zum Beispiel kann ich ihn auch kleiner einstellen, als ich will.
Und dementsprechend...
kann ich ihm Geld sparen.
Also, mal mal Resource.
Google Container Node Pool.
Heißt...
Wie habe ich den Cluster genannt?
Primary Pool oder wie soll ich das Ding nennen?
Pool. Ich nenne es einfach Pool. Fertig.
So, das Ding kriegt wieder einen Namen.
Dann muss man natürlich sagen, für welchen Cluster der Pool ist.
Für unseren Primary Kubernetes Cluster.
Wir sind mal großzügig und sagen Node Count 3.
Das funktioniert so standardmäßig nicht.
Wie gesagt, ich habe zu wenig Festplattenplatz.
Ihr seht, fünf Minuten, das Ding ist immer noch nicht angelegt.
Ich habe zu wenig Festplattenplatz.
Deswegen muss ich jetzt noch eine Node Config angeben.
Und zwar unter anderem Disk Size Gigabyte.
Da machen wir 40.
Das heißt, er legt drei Nodes an.
Mit jeweils 40 Gigabyte SSD Platz.
Heißt im Endeffekt 120G SSD Speicher.
Das passt ohne Probleme in meinen Quota rein.
Gar kein Ding.
So, jetzt können wir noch ein paar andere Sachen angeben.
Nämlich, dass das Ding sich Auto-Upgraden soll und automatisch reparieren soll.
Habe ich auch aus dem Tutorial.
Keine Ahnung, was es genau macht.
Aber automatisches Reparieren und nachts nicht angerufen werden,
ist prinzipiell schon mal ganz nice.
Service-Account können wir angeben in der Node Config muss man das, glaube ich, machen.
Service-Account können wir den Kubernetes-Account angeben,
den wir vorhin angelegt haben.
So, Machine-Type für die Nodes.
Wir wollen es möglichst billig haben.
E2-Small, das ist das, was irgendwie 15 Euro oder so im Monat kostet.
Also im Prinzip kostet uns das gesamte Setup irgendwas um 70 Euro im Monat oder so.
Also da sieht man, so ein Managed-Kubernetes-Cluster bei dem großen Cloud-Anbieter
muss nicht zwangsläufig günstig sein.
Ich würde mal sagen, so ungefähr 70 Euro im Monat wird uns der ganze Spaß kosten.
Ohne jetzt großartig Storage-Preise und Traffic-Preise und sowas dabei.
Also das kann schon recht teuer werden.
Also wer auf die Idee kommt, Cloud ist immer billiger,
würde ich so pauschal nicht unterschreiben.
So, und noch was aus dem Tutorial, wo ich auch nicht weiß, was das macht.
Das ist irgendwelche Berechtigungen.
Worauf die Nodes zugreifen dürfen.
Das war's. Mehr brauchen wir nicht für unseren Kubernetes-Cluster.
Für den Node-Pool, wenn wir... Alter, wie lange brauchen das bitte zum Anlegen?
Das ist ja Resident Sleeper, Mann.
Wie gesagt, ich habe euch gewarnt, das ist ultra lahm teilweise.
Also Kubernetes-Cluster erzeugen, da drehst du teilweise 10 Minuten Däumchen.
Der ist wirklich schon ins Bett ge... Still creating.
Alles Arbeitszeit, genau, so sieht es aus.
Eats Money, alles abbrechen, na klar.
Na, ich werde garantiert einstechen, bevor ich ein Kubernetes-Cluster aufbaue.
Erst mal schön Home Office startet, einstechen, Zeit eintragen
und dann Kubernetes aufbauen.
Nicht vorher.
Also ich habe das vorhin noch mal ausprobiert, da hat es nur 5 Minuten gebraucht.
Aber es könnte sein, dass in einem neuen Projekt das Ganze langsamer geht.
Ach, guck mal, er hat den Node mittlerweile gelöscht.
Das heißt, er müsste fast durch sein.
Er ist fertig, easy, yeah, nice.
Hat ja nur 9 Minuten 30 gebraucht.
So, mein Kubernetes-Cluster ist running.
Aktuell total unnütz, Node-Number 0, total CPU...
Also ein Kubernetes-Cluster hat quasi eine Hülle ohne Nodes.
Das bringt mir natürlich nichts.
Das heißt, jetzt applye ich mal noch mal Terraform mit meinem Node-Pool.
Und jetzt sollte ich ein Kubernetes-Cluster bekommen,
wenn ich alles richtig gemacht habe, mit drei Nodes, jeweils 40 Gigabyte SSD-Speicher,
und ja, jetzt nicht die fetteste VM.
Ich glaube, ein CPU-Core pro Node.
Aber zum Testen reicht das, ja.
So, Terraform apply.
Mal gucken, was Terraform anzeigt, was es machen möchte.
Schadet nichts, sich das immer mal anzugucken.
Fuck, Alter, wo ist das Problem?
Incorrect attribute value type.
Cool.
Cluster, Container-Cluster-Primary.
Habe ich wieder Self-Link oder was vergessen?
Cluster.
Das muss man machen.
Cluster to create a Node-Pool for.
Cluster must be present in location...
Okay, Self-Link vielleicht.
Gucken wir mal, ob das jetzt funktioniert.
Also es ist nicht unbedingt immer alles so hundertprozentig klar,
was man da einstellen muss.
Deswegen Fall...
Alter.
ID.
Das heißt, falls ihr bei eurem Arbeitgeber so was aufsetzen müsst,
lasst euch auf jeden Fall eine Spielumgebung...
geben, um da ein bisschen rumzuprobieren.
Das Schöne an dieser Terraform-Geschichte ist,
wenn es einmal angelegt ist hier und es funktioniert,
dann funktioniert es immer reproduzierbar.
Und das ist ja auch eines der großen Selling Points,
wie man dann so schön sagt,
für das du es reproduzierbar immer wieder so aufbauen kannst.
Okay, das war jetzt anscheinend richtig mit der ID.
Es wäre natürlich sinnvoll, wenn hier stehen würde,
the Cluster ID to create a Node-Pool for.
Aber nein, obviously muss da ID rein,
so wie man das selbstverständlich
ganz einfach erkennen konnte.
So, jetzt wird eine Node-Pool erstellt.
Der Node-Pool zu erstellen dauert noch einmal fünf Minuten.
Das ist alles nicht so schnell.
Wenn wir Glück haben,
hat unser Quota mittlerweile wieder aktualisiert auf null runter.
Nee, hat noch nicht aktualisiert.
Ah, gut.
Debated.
So, also Leute, ich habe ungefähr noch eine halbe Stunde Zeit.
Ich zeige euch jetzt noch eine coole Sache.
Ach ja, Frage, Chat.
Soll ich dieses Repo auf GitHub hochpushen,
dass ihr abgucken könnt?
Du, du, du.
Trinken, sehr gut, mache ich jetzt.
So, okay.
Dann machen wir schon mal ein GitHub.
Ah, Moment, da müssen wir auf Firefox aufmachen.
Auf Firefox sind wir eingeloggt.
GitHub.
GitHub.
Repo, New Repo.
In der Pfeife der Orga natürlich.
Ah, komm, wo bloß?
K8SG...
GCP...
Ja, nennen wir es mal so.
Wie ist das so im neuen Job?
Sehr viele Abkürzungen
und ich blicke noch bei ganz vielen Sachen nicht durch.
Keine mit Zugangsdaten.
Ja, ja, natürlich.
Ja.
Aber seit Ende letzter Woche ist besser.
Ich habe jetzt wenigstens was, was ich machen kann.
SSH-Login geht übrigens immer noch nicht nach zwei Wochen.
Wie viel der Config kannst du wiederverwenden,
wenn man den Hoster wechselt?
Ehrliche Meinung, ehrliche Antwort?
Gar nichts.
Dafür ist Terraform nicht gedacht.
Terraform abstrahiert nicht zwischen Cloud-Anbietern,
sondern macht den Aufbau bei einem Cloud-Anbieter reproduzierbar.
Ja, und mit State-Tracking und alles.
Wenn du jetzt umziehen willst nach Azure,
ist die Config komplett anders.
Ich meine, allein schon, dass du hier oben drinnen stehen hast,
Resource Google-Container-Node-Pool.
Das heißt, bei Azure bestimmt nicht so
und hat bestimmt noch andere Settings.
Nichts kannst du davor verwenden.
Gar nichts.
No pass.
Haben wir überhaupt schon ein Repo angelegt?
Terraform-Git-Ignore.
Gibt es auch bestimmt ein gutes Git-Ignore-Template.
Wobei, wir haben eh kein State.
Vim-Git-Ignore.
Status-Git-Init.
Git-Status-Terraform.
Ja, ja, okay.
Git-Add.
Status, ja, okay.
Der checkt den ganzen Müll nicht mit ein.
Alles gut.
Git-Remote-Add.
GitHub.
Boom.
Okay.
Google-to-Azure-Converter.
Also, ich könnte mir vorstellen,
dass es tatsächlich Konvertierungsprojekte gibt.
So eine Cloud-Infrastruktur,
Terraform-Config und eine andere.
Wie gut das funktioniert,
ich habe da meine Zweifel.
Moment, ist er fertig?
Terraform-Stells?
Ja, ist fertig.
Easy, hat funktioniert.
Also, unser Node-Pool ist fertig.
Okay.
Also, unser Node-Pool ist angelegt.
Wir haben einen Kubernetes-Cluster in der Google Cloud am Start
mit 3 Nodes, 6 CPUs und 6 GB RAM.
Wir können jetzt mal kurz in die Übersicht reingucken.
Wir sehen, okay, wir haben sogar ein Kubernetes-Upgrade available.
Was haben wir für ein Upgrade available?
Wir können auf den Rapid-Channel gehen.
Dann kriegen wir diese Dinger hier zur Auswahl.
Wir können auf den Regular-Channel gehen.
Da kriegen wir die hier zur Auswahl.
Oder wir gehen auf den Stable-Channel.
Dann gibt es die 25 noch gar nicht.
Verstehe.
Tja, aber wir machen jetzt keine Updates.
Bist du eigentlich als typischer Programmierer,
der eigentlich nur programmiert?
Ich?
Nee, ich arbeite nicht als Software-Entwickler.
Ich mache was ganz anderes.
Ich mache eher sowas hier.
Also, beim alten Arbeitgeber war ich DevOps-Engineer,
beim neuen bin ich Senior Linux-Engineer.
Ich bin nicht, ich bin, ich war noch nie
hauptberuflicher Software-Entwickler
und habe ich eigentlich auch gar nicht vor.
Ich weiß auch nicht, ob mein Programmier-Stelz
dafür reicht, ob ich das machen könnte überhaupt.
Habe ich aber ehrlich gesagt überhaupt keinen Bock drauf.
Ich finde den Bereich, wo ich jetzt bin,
ist eigentlich genau das Richtige.
Und gerade die ganze Cloud-Geschichte ist was,
was mir auch echt Bock macht.
Und was wir sehen, wir haben kein external Endpoint
vom Kubernetes-Cluster.
Das heißt, das ist nicht erreichbar aus dem Internet.
So, und jetzt zeige ich euch noch mal was sehr Nices.
Naja, im DevOps-Bereich, du kannst aus mehreren Ecken
kommen, also klassischer DevOps bedeutet ja,
dass Entwickler sich auch ein bisschen über den Tellerrand
hinausschauen und Sachen, die sie entwickelt haben,
auch selbst betreiben.
Das ist so das, wo DevOps eigentlich herkommt.
Mittlerweile hat sich dieses Wort eher, eher selbst verflüchtigt,
dass es quasi ganz eigene DevOps-Teams gibt,
beziehungsweise auch ehemalige Admins und Plattform-Architekten.
Und hast du nicht gesehen,
die sich um Tool-Bereitstellung, Tool-Verwaltung kümmern
und man da irgendwie so den Stempel DevOps draufdrückt.
Ja, also das hat sich so ein bisschen verflüchtigt, das Wort.
Also ein bisschen selbstständig gemacht, das ist das DevOps-Wort.
Das heißt, nur wenn du, du brauchst keinen großen Hintergrund
in Software-Entwicklung.
Es ist allerdings von Vorteil meiner Ansicht nach,
dass man ein bisschen Ahnung von Software-Entwicklung hat
im DevOps-Bereich.
Oftmals darum, Bild-Tools bereitzustellen,
Bild-Prozesse bereitzustellen für Software
oder Tools für Entwickler bereitzustellen
oder irgendwelche Continuous Delivery, Continuous Integration,
Build-Sachen zu bauen.
Das heißt, es ist nicht verkehrt, sich ein bisschen auch
in die Entwickler reinversetzen zu können.
Aber du musst jetzt vorher, du musst jetzt vorher nicht irgendwie
Hauptberuf für Software-Entwickler gearbeitet haben,
um heute in den DevOps-Bereich reinzukommen.
Weil DevOps heute quasi was Eigenständiges ist.
Es hat sich quasi,
verselbstständigt.
Ja, das kommt, ja, das stimmt.
Na, komm, jeder Admin, der was auf sich hält,
nennt sich doch heute auch DevOps-Engineer.
Guck mich an.
Ist doch fast schon normal.
Deswegen habe ich ja auch gerade gesagt, wo es herkommt
und was man heute zum größten Teil darunter versteht.
Herkommt, habe ich ja gerade gesagt, eher aus dem Software-Entwicklungsbereich,
dass dort die Software, die man programmiert hat,
auch selbst verwaltet, selbst betrieben wird.
Dass man sich auch ein bisschen um Operations kümmert.
Dass man quasi ein Dev ist,
die halt die Operations macht.
Aber wird ja mittlerweile im praktischen Alltag eher,
wurde wie gesagt, das ist ja eher,
hat sich ja verselbstständigt das Wort,
dass es jetzt auch eigene DevOps-Teams und sowas gibt.
Also du stellst die programmierte Software bereit
und schaust, dass die auf dem Server oder so läuft oder so.
Ja, also es ist jetzt vielleicht nicht ganz so geschliffen formuliert,
aber das ist auf jeden Fall Teil dessen,
was in vielen DevOps-Teams gemacht wird.
Du schaust, dass es automatisierte Build-Prozesse gibt für die Software.
Du schaust, dass es beispielsweise auch sowas gibt wie automatisierte Tests,
automatisierte Build-Workflows, automatische Handling von Pull-Requests.
Also zum Beispiel, wenn ein Pull-Request erstellt wird,
dass eine Testumgebung erstellt wird,
dass der Pull-Request da drin nochmal temporär gemerged wird,
Tests durchgeführt wird,
dann, dass es ein Bot gibt, der den Pull-Request reinschreibt.
Jawohl, Tests sind erfolgreich gelaufen.
Das ist auch immer sowas in der Richtung,
also automatische Build-Pipelines.
Und letztendlich gehört auch oftmals mit dazu,
dass das, was gebaut wurde, dann deployed wird am Ende.
Also das heißt ja nicht umsonst Continuous Integration und Continuous Delivery.
Und der Continuous Delivery, das CD, wo das CD steht,
wie gesagt Continuous Delivery,
das ist das, was man allgemein unter Deployment quasi versteht.
Also Continuous Integration,
du fügst permanent neue,
du nimmst Pull-Requests, fügst permanent neue Features hinzu,
möglichst schnelle Iterationen, wie man das so schön nennt,
und baust das, baust neue Versionen und die gebauten Versionen deployst du dann
und das ist der Continuous Delivery Teil.
Es ist alles ein bisschen schwammig.
Ich kann euch das auch nicht besser erklären.
Außerdem verstehen auch viele Unternehmen alle wieder ein bisschen was anderes darunter.
Und ja, ja, das stimmt schon.
Man muss sich, je nachdem, wo du mal liest,
muss man sich natürlich nach dem richten, was gewollt ist.
Ich habe aktuell auch meine Probleme mit meinem Windows Notebook
und bin schon am überlegen, wie ich mich am besten irgendwie durchtunnel,
ohne dass es illegal wird.
Also insofern, ja.
Ja, also ich wollte euch jetzt ja noch was zeigen.
Wie gesagt, ich habe noch 20 Minuten Zeit.
Ich zeige euch jetzt noch was wirklich sehr Nices.
Wir pushen, wir pushen das Ganze erstmal.
Dieses Inet-Skript könnt ihr eigentlich vergessen.
Git, Commit, warte mal, Add.
Haben wir schon geadded?
Add.
Git, Commit, minus A, minus M, Inet.
Git, Push, Push, GitHub, Master.
Oh, Master.
Leute, bitte nicht Twitter Bescheid sagen.
So, und jetzt gehen wir mal auf GitHub und in meinem Repo ist es drinne.
Also falls sich jemand angucken will, was ich heute fabriziert habe,
dann kann er sich das hier angucken.
Wobblos, K8S, GCP.
Es ist noch nicht ganz fertig.
Ich kann noch ein bisschen Werbung machen für den Stream,
beziehungsweise auch vielleicht für meinen YouTube-Channel.
Sie können ja mal bei mir vorbeigucken auf YouTube, auf Twitter und hier.
Followen, wenn es euch gefällt, auf Twitch.
Denn wir machen die nächsten Tage das Projekt noch ein bisschen weiter.
Das Ziel ist ja, dass wir am Ende einen funktionsfähigen Kubernetes-Cluster haben.
In der Google Cloud, auf dem wir auch zwei Demo-Anwendungen deployen.
Auch public zugänglich, also ihr könnt dann im Internet auch drauf.
Dass wir dann Zertifikatsmanagement dabei machen, automatisches Deployment drauf machen.
Und das eben ordentlich einrichten.
Und das Ganze auch Security-technisch mit verschlüsselter Kommunikation im Cluster mit einem Service-Mesh aufbauen.
Weil das aktuell gerade so der neueste Shit ist.
Oh, guck mal.
Ich habe meinen Terraform-Log auch noch gepusht.
Ist das gut oder schlecht?
Ist wahrscheinlich gar nicht so wild.
Dann sollte man das Terraform-Log einchecken oder nicht?
Da ist auf jeden Fall drinne die Version und sowas sind da geloggt.
Ich weiß jetzt nicht, ob man das normalerweise mit eincheckt oder nicht.
Gucken wir mal, was das gitignore sagt.
Log, ja, das wird mit eingexeckt.
Ja, sollte man.
Gut, dann ist gut.
Ja, es stimmt, es ist sinnvoll, das einzuchecken, weil es könnte ja sein, dass bei der nächsten Version andere Sachen unterstützt werden und es gar nicht mehr funktioniert.
Ja, ja, okay, ist eigentlich logischer.
So, also.
Da haben wir unser Kubernetes-Cluster laufen.
Also folgt mir gerne auf Twitch.
Ich würde mich sehr freuen, wenn ihr die nächsten Streams auch wieder am Start seid.
Wir basteln hier dran weiter, sodass wir im Endeffekt einen super konfigurierten, gut lauffähigen, automatisch deployenden Kubernetes-Cluster am Start haben.
In der Google Cloud.
So, jetzt wollte ich euch noch eine andere Sache zeigen.
Und zwar, ich habe jetzt meinen Kubernetes-Cluster.
Der läuft jetzt, den seht ihr ja auch hier.
Ja, da.
Der ist am Start.
Wie komme ich denn jetzt da dran?
So, das ist jetzt der eigentliche Knackpunkt.
Wie komme ich denn jetzt da dran?
Gar nicht so einfach.
Es gibt hier so ein Hilfskommando, connect.
Bringt mir allerdings nicht so viel.
Vergiss das nicht wieder zu löschen.
Oh ja, sonst ist mein Guthaben schnell weg.
Also, ich kann hier...
Ah, okay.
Ich muss irgendwas installieren für meine Google-Command-Line-Tools.
Wie macht man das?
Wie installiert man da was?
Install.
Google-Components-Install-Auth-Plugin.
Die haben übrigens einen mega hässlichen Progress-Bar hier unten.
Weiß nicht, was die Google-Leute sich dabei gedacht haben, dass das hübsch ist.
Ich weiß ja nicht.
Marzilein sagt, der Auer.
War letztens bei Hetzner und hat darüber Videos gemacht.
Oft hat man da viele einzelne Systeme gesehen.
Ich habe das so verstanden, dass diese einzelnen Systeme vermietet werden.
Gibt es aber nicht auch die Form, dass man nur einen Teil eines größeren Systems oder ganze Cluster mietet und sich das je nach Lust und Last verteilt?
Glaube du erst.
Das gibt es beides, ja.
Was heißt beides?
Das ist ein fließender Übergang.
Also, was es gibt.
Wir machen das jetzt mal ganz schnell.
Was es gibt.
Das erste, was es überhaupt gab, war ein dediziertes Server.
Das heißt, du hast hier so einen Server, den du da siehst, hast du dir gemietet.
Der gehört dir.
Nur, dass er bei Hetzner im Rechenzentrum steht.
Der gehört dir.
Da kannst du alles drauf machen, was du willst.
Ich glaube, man kann sogar ins BIOS.
Oder?
Kann man sogar ins BIOS?
Ich weiß gar nicht.
Ich glaube schon.
Es ist dein Server.
Das ist in den letzten Jahren ein bisschen außer Mode gekommen.
Weil, man muss sich um alles selbst kümmern und es ist auch nicht hoch verfügbar.
Es ist halt einfach ein Server.
Ohne irgendwas.
Einfach ein nackiger Server.
Und wenn die Festplatte kaputt ist, ist er kaputt.
Man muss sich selbst um Backups und alles kümmern.
So, also das ist das eine.
Das gibt es schon ganz lang.
Heutzutage ist das Bare Metal, nennt man das, wenn man cool sein will.
Früher hat man mal das Dedicated Server genannt.
Heute ist das Bare Metal.
Der nächste Schritt war, dass die...
Hoster VMs angeboten haben.
VMs, das sind virtuelle, virtuelle Server, die auf irgendeinem Virtualisierungscluster laufen.
Das sind dann quasi mehrere dieser Kisten, die du siehst, zusammengefasst zu einem Virtualisierungscluster.
Und darauf laufen dann potenziell aber mehr VMs, als du dort Server siehst.
Also mal angenommen, du siehst dort, du hast drei fette Server.
Darauf kannst du ohne Probleme 200 VMs laufen lassen.
Und die werden dann vermietet.
Allerdings, das ist, glaube ich, das so ein bisschen auch, was du meinst.
Allerdings da natürlich von der Leistung her auch nur anteilig.
Also du kannst zwar 200 VMs drauf machen, aber wenn du unter der Haube in dem Cluster nur drei Nodes mit jeweils acht CPU-Kernen hast, kannst du halt...
Du kannst zwar VMs vermieten mit 800 CPUs.
Wenn aber alle deiner Kunden gleichermaßen Lastursachen da drin, dann teilen die sich die Infrastruktur unten drunter.
Das ist wahrscheinlich das, was du meinst.
Und dann gibt es natürlich noch den neuesten Shit, dass man gar keine Server mehr mietet und auch keine VMs mehr mietet, sondern sich Services mietet.
Wie zum Beispiel das, was wir hier heute gemacht haben.
Wir haben ein Kubernetes-Cluster angelegt in der Google Cloud.
Und ja, letztendlich irgendwo auf irgendwelcher Hardware läuft das auch.
Die Hardware steht in Frankfurt, mehr weiß ich dazu aber nicht.
Ich weiß nicht, welche Hardware das ist.
Doch, kann ich nachgucken, aber es muss mich nicht interessieren.
Welche Hardware es ist, wie das verwaltet wird, kann mir vollkommen egal sein.
Ich bekomme mein Kubernetes-Cluster und kann mit den passenden Tools darauf zugreifen.
Ist natürlich noch ein bisschen teurer, aber ich muss mich auch weniger drum kümmern.
So, jetzt wird euch vielleicht was auffallen.
Wir möchten jetzt ja zum Beispiel sowas machen wie cubectl get pods-a.
Und jetzt werdet ihr sehen, hm, das geht ja gar nicht.
Und Chat, wer hat mitgedacht?
Warum geht mein cubectl nicht, obwohl ich mich gegenüber meines Clusters authentifiziert habe?
Werden aufgepasst, warum geht das nicht?
Chat enttäuscht mich nicht.
HiIQ.
Warum komme ich da nicht hin?
Du musst erst auf den Jump-Post, richtig.
Mein Cluster ist nicht public.
Ich habe keinen public Endpunkt.
Ich müsste quasi zu 1.7.2.16.0.2, äh 16.0.2, komme ich aber nicht hin, weil mein Cluster nicht public ist im Internet.
Das heißt, ich brauche einen Jump-Post und dafür habe ich mir was gebastelt.
So, die offizielle Variante, wie man sowas macht, ist, also das Einfachste wäre das hier.
Das Einfachste wäre, wir machen ein, wir loggen uns per SSH ein und machen dann hier, habe ich noch nicht mal entstehen,
würden dann hier auf der VM, wo wir per SSH, also das ist die VM, die hier in der Google Cloud läuft,
und würden unsere cubectl-kubernetes-Administration darüber machen.
Das wäre möglich, ist ein bisschen blöd, ist eine extra Kiste, müssen wir alles neu entstehen.
Wir müssen jetzt, wir müssen jetzt eher gucken, wie können wir uns tunneln zu dem Kubernetes-Cluster über die VM.
Da habe ich mir ein paar, da habe ich mir was zu überlegt.
Also, die schönste Variante ist ein VPN, ja.
Man kann ein VPN einrichten in der Google Cloud.
Es ist sogar, glaube ich, WireGuard-kompatibel.
Man könnte WireGuard benutzen.
Dann baut man ein VPN auf, quasi in seine, in das private Netz, in seiner Google-Infrastruktur und kann sich dann dahin verbinden.
Da muss man aber VPNs einrichten und hier und bla.
Wenn man einfach nur cubectl ausführen will über den Jumppost, gibt es, gibt es einfachere Varianten.
Nämlich, man kann das hier machen.
Ich zeige, ich zeige euch jetzt nur mal die, die, die Kurzvariante.
Man kann das hier machen.
Ich compute SSH auf den Jumppost und dann minus D1337, das ist der Port.
Quieting, sonst wie so.
Was macht das Ganze?
Das baut, und jetzt ist, das ist wirklich extrem Six Head.
Äh, Moment.
Export, HTTPS, SOX, Proxy, wie, wie macht man das jetzt?
Das...
So, und was macht das jetzt?
Das SSH verbindet sich auf meinen Jumppost, auf meine VM, durch den normalen Google-Tunnel.
Macht bei mir lokal ein SOX-Proxy auf.
Und wartet einfach da drauf.
Das heißt, wenn ich jetzt sage hier...
Export, HTTPS-Proxy, SOX 5, Localhost, Port 1337.
Und machen wir mal HTTP auch.
Und jetzt sage ich cubectl, cubectl get ports.
Haha, es funktioniert.
Das ist wirklich das, das billig VPN.
Also, wenn ich keinen Bock habe, Google Cloud VPN einzurichten, ich möchte mich einfach nur tunneln.
Das ist ein Tipp, das könnt ihr, das könnt ihr überall drauf anwenden.
Das ist ja ein ganz normales SSH-Kommando unter der Haube.
Ja, das sind die SSH-Parameter.
Ähm...
Das ist halt sehr nice.
Das heißt, ihr könnt euch jetzt mal auf die Schnelle in eure Cloud-Infrastruktur verbinden, die ja, die ja wirklich nur lokal erreichbar ist, eigentlich.
Innerhalb dieses Netzes in der Google Cloud.
Und...
Ihr könnt drauf zugreifen.
Ihr könnt allerdings nicht mehr machen als cubectl in dem Fall.
Äh, ich meine, ihr könntet, ihr könnt noch auf andere Server zugreifen an der Stelle, wenn ihr mit einem Browser einen SOX-Proxy eintragt.
Es ist allerdings nicht so schön.
So, und ich habe mir jetzt noch eine, eine kleine Erweiterung dazu ausgedacht.
Wie gesagt, ich sage euch gleich dazu, ich empfehle euch nicht, das im produktiven Umfeld zu benutzen.
Dann würde ich tatsächlich ein VPN aufsetzen.
Aber ich habe noch was anderes gebaut.
Und ich zeige euch jetzt mal ein Six-Head-Script.
Was ich mir überlegt habe.
Tunnel.sh.
Bam.
Zack.
Was macht das?
Ich führe es erstmal aus, okay?
Ich führe es erstmal aus.
Wir gucken, ob es funktioniert.
Und dann, und dann erkläre ich euch schnell nochmal, wie es funktioniert.
Oh, ich bin doof.
Ich habe das, ich habe das Exit nicht rausgenommen.
Lul, weh.
So, sudo.
So, und wir haben einen Tunnel am Start.
Oh, fuck.
Irgendwas ist kaputt.
Kackel, ich weiß nicht warum.
Was ist kaputt?
Oh nein.
Irgendwas ist kaputt.
Shit.
Warum?
Okay, wir müssen mal ein Script schnell fixen.
Was ist, was ist denn hier kaputt?
Was habe ich, was habe ich kaputt gemacht?
Okay, ich kann euch, ich kann euch mal kurz erklären, was das macht.
Ist das gerade deine Public IP?
Das hier?
Ne, das ist nicht meine Public IP.
Das und das auch nicht.
Also, ähm.
Was der machen sollte, ist folgendes.
Schon Urlaub im neuen Job.
Oder heute, heute habe ich freien Tag, ja.
Also, ähm.
Ich, ich, ich, ich erkläre euch mal, was das macht.
Das holt sich, das holt sich mein Default Interface und die Projekt ID.
Dann lädt sich ein, äh, SOX Proxy Tunnel Tool runter von Github.
Was von irgendwelchen Chips kommt.
Was von irgendwelchen Chips kommt.
Was von irgendwelchen Chips kommt.
Was von irgendwelchen Chips kommt.
Was von irgendwelchen Chips kommt.
Was von irgendwelchen Chips kommt.
Was von irgendwelchen Chips kommt.
Was von irgendwelchen Chips kommt.
Was von irgendwelchen Chips kommt.
Was von irgendwelchen Chips kommt.
Was von irgendwelchen Chips kommt.
Was von irgendwelchen Chips kommt.
Was von irgendwelchen Chips kommt.
Was von irgendwelchen Chips kommt.
Was von irgendwelchen Chips kommt.
Was von irgendwelchen Chips kommt.
Was von irgendwelchen Chips kommt.
Was von irgendwelchen Chips kommt.
Was von irgendwelchen Chips kommt.
die falschen IPs. Kann das sein?
Was für IPs habe ich genommen?
Ich habe andere
IPs genommen, als in meinem
Testaufbau. Äh, Network?
Ah, 24.
Exzellent.
Daran wird es wahrscheinlich liegen.
Ich habe andere IPs genommen.
Machen wir hier nochmal ein Sleep davor.
Was ist mit dem Stackoverflow Darkmode?
Passiert jetzt mein Darkreader. Der macht komische Dinger.
Der Darkmode
vom Chrome macht komische Sachen.
So.
Und jetzt sollte das eigentlich
funktionieren. Ich hoffe mal, dass ich das richtig gemacht habe.
Brage, Leute. Brage.
Please, brage.
Brage. Brage.
Ach,
shit, Alter.
Das funktioniert nicht.
Was habe ich denn kaputt gemacht, Mann?
Warum geht mein Six-Head-Skript nicht?
Was ist das?
Warum geht das nicht?
Warum geht das nicht? Ich habe keine Ahnung.
Was meckert
er denn?
Was meckert
er denn hier?
Connect? No?
Was?
Lookup local? Wait a minute.
Was ist denn das für ein Fehler?
Moment. Ping?
Local host?
Moment.
Was ist denn da?
Wait a minute.
Sollte da nicht mehr
drinne stehen?
Wait a minute.
Warum ist mein ETC-Host kaputt?
Haben wir da letztens irgendwas verkackt?
Kann das sein?
Äh, Arch-Local-Hosts.
Äh, äh, Arch-ETC-Hosts.
Deleted.
Deleted my host.
Domain. Und der Host-Name
muss da noch hin. Warum?
Warum? Wie? Ich habe Host-Name, die jetzt hier...
Äh.
Äh, Pac-Man-QL-Host-Name.
Äh, wie heißt das?
Minus Q-F?
Ach, fuck.
Ähm, Arch-Install-Host-Name.
Host-Name, ja.
So.
Host-Name, Command-Out-Found.
Ja, was muss ich installieren?
Sag's mir.
Inet-Utils.
Obviously, alles klar. Das weiß man ja.
Inet-Utils.
Rust-Beste.
Ey, Rust ist eine coole Sprache.
Allerdings wird sie oftmals
für Sachen verwendet, wo ich sie nicht so sinnvoll für finde.
Host-Name, Stream.
Stream.
Stream.
Mein Script ist richtig, mein Host-File ist kaputt.
Ping, Local-Host.
Äh, ist es jetzt besser?
Ping, Stream.
Ich hoffe mal, das ist besser jetzt.
Tunnel.
Komm, Brage, Leute.
Brage, Brage, Brage.
Ah, es funktioniert.
Das war meine Host-Datei, die kaputt ist.
Geil.
So, und jetzt kann ich einfach Cube-CTL machen.
Über diesen Proxy.
Das ist wirklich richtig...
Poor-Mans-VPN.
Was hältst du von C++?
Jetzt nicht, Leute. Ich muss gleich weg in drei Minuten.
Also, mal ganz schnell, was dieses Skript macht.
Das baut ein VPN auf, ohne ein VPN zu brauchen.
Also.
Folgendes.
Es holt sich das Default-Interface raus.
Von meinem, von meiner Kiste.
Es holt sich die Projekt-ID raus, aus meinen Terraform-Variablen.
Dann lädt es sich so ein Tunnel-Tool runter, hier von GitHub.
So ein China-Chinesen-Tunnel-Tool.
Das da.
Was eigentlich dafür gemacht ist, um an der China-Chinesen-Firewall vorbeizugehen, aber okay.
Geht für unsere Zwecke auch.
Lädt sich ein Tunnel-Tool runter, was UDP- und TCP-Traffic über SOX-Proxys tunneln kann.
Dann baut es einmalig eine Verbindung auf zum Google-Cloud-Jump-Host.
Lockt sich nur ein, lockt sich gleich wieder aus.
Der Sinn dahinter ist, dass SSH-Keys generiert werden.
Dann legt er ein Tunnel-Device an, gibt dem Tunnel-Device eine IP, startet das Tunnel-Device, setzt Routen für unsere Routen in Google-Cloud.
Nämlich hier das Netz, was wir benutzen und hier das für Kubernetes.
Und dann startet er dieses Tun-to-SOX-Tool über den SOX-Proxy, den ich hier unten aufbaue über SSH.
Und dann schickt er das in den Hintergrund und dann startet er den SOX-Proxy über SSH.
Also im Prinzip.
Im Prinzip ist das quasi ein SSH-Tunnel mit SOX-Proxy, über den TCP- und UDP-Traffic getunnelt wird.
Das ist nur was für richtige Aluhutträger, die sowas machen.
Nee, hat damit ja nichts zu tun.
Aber es funktioniert und man muss kein VPN einrichten.
Man kann einfach sagen .slash Tunnel und ihr seht, kubectl geht noch nicht.
Und sobald das VPN aufgebaut ist, das fake, billig VPN aufgebaut ist, kubectl funktioniert.
Ist das nicht pock?
Wie gesagt, nicht unbedingt für Production-Usage gedacht, aber so an sich eigentlich ganz nice.
So, das packen wir noch in unser Git-Ignore-Status.
Comet 0w, beste Push-GitHub-Master.
So, und jetzt habt ihr auch den ganzen Campbell auf GitHub.
Ich gucke jetzt gleich nochmal in Chat.
So, also hier habt ihr die Sachen von heute im Stream als Repo.
Dann machen wir das nächste Mal weiter.
Das heißt, wenn ihr dabei sein wollt, Leute, folgt mir einfach auf Twitch.
Aber das nächste Mal weiter.
Also, das habe ich beantwortet.
Dafür gibt es ja mittlerweile Bots, die Abhängigkeiten scannen.
Ersetzt zwar nicht jemand, der wirklich Mailing-Listen liest und sowas, aber es ist schon mal nice.
Ich habe montags immer frei.
Weil ich nur vier Tage die Woche arbeite.
Ruby!
Ruby ist nice.
Ruby ist meine Lieblingssprache, Nebasi Sharp.
Leider ist Ruby so unbedeutend geworden.
Die letzten Jahre über.
Ey, Max, hoffe alles so gut.
Ja, nach gestern, heute ist wieder alles gut.
Ja.
Ich wollte mal fragen, ob du bald wieder auf YouTube ein paar Worte zu Battlefield verlierst.
Oh.
Bestimmt.
Bestimmt.
Bestimmt.
An das Trinken erinnern.
Ja.
Mach ich.
Trinken.
Okay, Leute.
Ich hoffe, euch hat der Stream gefallen.
Das war heute zu einer Zeit, ich weiß, es ist schwierig zu gucken für euch.
Sieht man auch an den Zuschauerzahlen.
Es war im Peak anstatt 200 noch was.
Heute Peak 150.
Aber es hat sich angeboten, das einfach heute mit nachzumachen.
Ich meine, die Worts bleiben da.
Leute, die es interessiert, können sich trotzdem angucken.
Und die nächsten Streams in der Richtung mache ich wahrscheinlich eher wieder abends.
Aber zumindest könnt ihr eurem Chef ein bisschen Fortbildung verkaufen.
150 Homeofficer, ja.
Also, ich gehe jetzt weg.
Ich muss noch wohin.
Wir sehen uns, Leute.
Bis dann, macht's gut.
Ich hoffe, euch hat's gefallen.
See you.
